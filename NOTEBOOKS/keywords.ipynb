{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ ME\n",
    "- This notebook includes the code used to extract keywords from the musoW and Music Journalism Insider databases. \n",
    "- The keywords form the basis of bigrams for searches across potential sources (Twitter, github etc...). \n",
    "- The code is also used to check keywords on negative training sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of results**\n",
    "\n",
    "MJI Bigrams:\n",
    "1. Oral History \n",
    "2. Music Magazine \n",
    "3. Sound Archive \n",
    "4. Music History/Culture/Research\n",
    "\n",
    "Useful unique KWs above 100:\n",
    "black/african american, interview \n",
    "\n",
    "musoW bigrams: \n",
    "1. Sheet Music\n",
    "2. Music/Digital Library/Collection\n",
    "3. Sound Recording \n",
    "4. Midi/Audio File \n",
    "5. Music Information \n",
    "6. Musical/Digital Score\n",
    "7. Song Dataset\n",
    "8. Digital Edition\n",
    "\n",
    "Useful unique KWs above 100:\n",
    "data/set/base, composer, edition, song, manuscript, score\n",
    "\n",
    "Relevant shared bigrams w/ most prominent set:\n",
    "1. Digital Archive/Library (musoW)\n",
    "2. Music Archive/Library (musoW)\n",
    "3. Music/Digital Collection (musoW)\n",
    "4. Archive Collection (mji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and set your path \n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import PlaintextCorpusReader, wordnet\n",
    "path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract top 35 keywords and return them alongside bigrams that include two words within top 35\n",
    "def keywords(input_corpus):\n",
    "    string = input_corpus.raw('')\n",
    "    tokenised = punct_tokenizer.tokenize(string)\n",
    "    clean = [w for w in tokenised if w not in stopwords]\n",
    "    clean_2 = [w for w in clean if w not in custom_stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in clean_2]\n",
    "    freqdist_lem = nltk.FreqDist(lemmatized)\n",
    "    most_common_lem = freqdist_lem.most_common(35)\n",
    "    most_common_lem_list = []\n",
    "    for t in most_common_lem:\n",
    "        most_common_lem_list.append(t[0])\n",
    "    bigrams = nltk.bigrams(lemmatized)\n",
    "    freqdist_bg = nltk.FreqDist(bigrams)\n",
    "    search_bigrams = []\n",
    "    for k, v in freqdist_bg.items():\n",
    "        if k[0] in most_common_lem_list:\n",
    "            if k[1] in most_common_lem_list:\n",
    "                if v > 5:\n",
    "                    k = ' '.join(k)\n",
    "                    search_bigrams.append([k, v])\n",
    "    kw_pd = pd.DataFrame(columns=['Most Common KW', 'KW Freq', 'Bigrams', 'Bigram Freq'])\n",
    "    kw_pd['Most Common KW'] = pd.Series([w[0] for w in most_common_lem])\n",
    "    kw_pd['KW Freq'] = pd.Series([w[1] for w in most_common_lem])\n",
    "    kw_pd['Bigrams'] = pd.Series([b[0] for b in search_bigrams])\n",
    "    kw_pd['Bigram Freq'] = pd.Series([b[1] for b in search_bigrams])\n",
    "    return kw_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: \n",
    "- extract needed info from MJI and musoW dumps \n",
    "- format, align and remove dupes\n",
    "- bounce to text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mji csv and turn into two column df for names and desc \n",
    "df_mji = pd.read_csv(path+'MJI/MJI_data.csv', keep_default_na=False, dtype='string')\n",
    "df_mji_small = df_mji.iloc[:, [0, 5]].copy()\n",
    "df_mji_small['Title'] = df_mji_small['Title'].str.lower().str.strip()\n",
    "df_mji_small['Description'] = df_mji_small['Description'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as above but for negative description set\n",
    "negative = pd.read_csv(path+'negative_set.csv', keep_default_na=False, dtype='string')\n",
    "negative['Title'] = negative['Title'].str.lower().str.strip()\n",
    "negative['Description'] = negative['Description'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read musow json dump and turn into df w/ same columns \n",
    "with open(path+'MUSOW/musow_name_desc_url_cat.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "musow_names = [result['name']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_desc = [result['description']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "df_musow = pd.DataFrame(columns=['Title', 'Description'])\n",
    "df_musow['Title'] = musow_names\n",
    "df_musow['Description'] = musow_desc\n",
    "df_musow = df_musow.astype('string')\n",
    "df_musow.to_pickle(path+'KEYWORDS/musoW_keywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates from MJI set based on title field \n",
    "df_mji_small[~df_mji_small['Title'].isin(df_musow['Title'])].dropna()\n",
    "df_mji_small.to_pickle(path+'KEYWORDS/MJI_keywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save each df to a single text file for processing \n",
    "with open(path+'KEYWORDS/mji_corpus.txt', 'a') as f:\n",
    "    dfAsString = df_mji_small.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)\n",
    "with open(path+'KEYWORDS/musow_corpus.txt', 'a') as f:\n",
    "    dfAsString = df_musow.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as above but for negative description set\n",
    "with open(path+'KEYWORDS/negative.txt', 'a') as f:\n",
    "    dfAsString = negative.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "- text processing using nltk: remove punctuations, tokenize, remove nltk stopwords + custom list, lemmatize \n",
    "- get top 35 keywords \n",
    "- get bigrams in which each word is in the top 35 kws "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set corpus variables + vars for cleaning \n",
    "mji_corpus = PlaintextCorpusReader(path+'KEYWORDS/mji_corpus.txt', '.*\\.txt')\n",
    "musow_corpus = PlaintextCorpusReader(path+'KEYWORDS/musow_corpus.txt', '.*\\.txt')\n",
    "negative_corpus = PlaintextCorpusReader(path+'KEYWORDS/negative.txt', '.*\\.txt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punct_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "custom_stopwords = ['available', '000', 'including', 'also', 'includes', 'website', 'new', 'include', 'well', 'based', 'source', 'sources', 'contains', 'search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process each and pickle \n",
    "musowkw = keywords(musow_corpus)\n",
    "musowkw.to_pickle(path+'KEYWORDS/musowkw.pkl')\n",
    "mjikw = keywords(mji_corpus)\n",
    "mjikw.to_pickle(path+'KEYWORDS/mjikw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as above but for negative training set, no pickle\n",
    "negativekw = keywords(negative_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3:\n",
    "- Analyze results, differences \n",
    "- Create reusable item for needed results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for differences\n",
    "musowkw['Source'] = 'musow'\n",
    "mjikw['Source'] = 'mji'\n",
    "different = pd.concat([musowkw, mjikw]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract kw and bigrams and export them to csv for analysis\n",
    "different_kw = different.copy()\n",
    "different_kw = different_kw[['Most Common KW', 'KW Freq', 'Source']]\n",
    "different_kw = different_kw.sort_values(by='KW Freq', ascending=False)\n",
    "different_kw.to_csv(path+'KEYWORDS/kw_summary.csv')\n",
    "different_bg = different.copy()\n",
    "different_bg = different_bg[['Bigrams', 'Bigram Freq', 'Source']]\n",
    "different_bg = different_bg.sort_values(by='Bigram Freq', ascending=False)\n",
    "different_bg.to_csv(path+'KEYWORDS/bg_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4:\n",
    "- run kw analysis on new training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pos_v4 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Description V4/Training Set Desc v4 - Positives.csv')\n",
    "desc_pos_v4 = desc_pos_v4.drop(labels=['Title', 'URL', 'Target', 'Source', 'Notes'], axis=1)\n",
    "with open(path+'KEYWORDS/desc_pos_v4.txt', 'a') as f:\n",
    "    dfAsString = desc_pos_v4.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_neg_v4 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Description V4/Training Set Desc v4 - Negatives.csv')\n",
    "desc_neg_v4 = desc_neg_v4.drop(labels=['Title', 'URL', 'Target', 'Source', 'Notes'], axis=1)\n",
    "desc_neg_v4['Description'] = desc_neg_v4['Description'].str.lower()\n",
    "with open(path+'KEYWORDS/desc_neg_v4.txt', 'a') as f:\n",
    "    dfAsString = desc_neg_v4.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_pos_v2 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Twitter V2/Twitter v2 - Positives.csv')\n",
    "twitter_pos_v2 = twitter_pos_v2.drop(labels=['Target', 'Source'], axis=1)\n",
    "twitter_pos_v2['tweet'] = twitter_pos_v2['tweet'].str.lower()\n",
    "with open(path+'KEYWORDS/twitter_pos_v2.txt', 'a') as f:\n",
    "    dfAsString = twitter_pos_v2.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_neg_v2 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Twitter V2/Twitter v2 - Negatives.csv')\n",
    "twitter_neg_v2 = twitter_neg_v2.drop(labels=['Target', 'Source'], axis=1)\n",
    "twitter_neg_v2['tweet'] = twitter_neg_v2['tweet'].str.lower()\n",
    "with open(path+'KEYWORDS/twitter_neg_v2.txt', 'a') as f:\n",
    "    dfAsString = twitter_neg_v2.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pos_v4_corpus = PlaintextCorpusReader(path+'KEYWORDS/desc_pos_v4.txt', '.*\\.txt')\n",
    "desc_neg_v4_corpus = PlaintextCorpusReader(path+'KEYWORDS/desc_neg_v4.txt', '.*\\.txt')\n",
    "twitter_pos_v2_corpus = PlaintextCorpusReader(path+'KEYWORDS/twitter_pos_v2.txt', '.*\\.txt')\n",
    "twitter_neg_v2_corpus = PlaintextCorpusReader(path+'KEYWORDS/twitter_neg_v2.txt', '.*\\.txt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punct_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "custom_stopwords = ['available', '000', 'including', 'also', 'includes', 'website', 'new', 'include', 'well', 'based', 'source', 'sources', 'contains', 'search', 'the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pos_v4_kw = keywords(desc_pos_v4_corpus)\n",
    "desc_pos_v4_kw.sort_values(by='Bigram Freq', ascending=False)\n",
    "desc_pos_v4_kw.to_csv(path+'KEYWORDS/desc_pos_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_neg_v4_kw = keywords(desc_neg_v4_corpus)\n",
    "desc_neg_v4_kw.sort_values(by='Bigram Freq', ascending=False)\n",
    "desc_neg_v4_kw.to_csv(path+'KEYWORDS/desc_neg_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_pos_v2_kw = keywords(twitter_pos_v2_corpus)\n",
    "twitter_pos_v2_kw.sort_values(by='Bigram Freq', ascending=False)\n",
    "twitter_pos_v2_kw.to_csv(path+'KEYWORDS/twitter_pos_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_neg_v2_kw = keywords(twitter_neg_v2_corpus)\n",
    "twitter_neg_v2_kw.sort_values(by='Bigram Freq', ascending=False)\n",
    "twitter_neg_v2_kw.to_csv(path+'KEYWORDS/twitter_neg_v2.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
