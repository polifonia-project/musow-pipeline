{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe function\n",
    "\n",
    "def append_to_pd(json):\n",
    "    \n",
    "    #username setup\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #1. username\n",
    "    author_id = [tweet['author_id'] for tweet in json['data']]\n",
    "    user = [username[id] for id in author_id]\n",
    "\n",
    "    # 2. Time created\n",
    "    created_at = [dateutil.parser.parse(tweet['created_at']) for tweet in json['data']]\n",
    "\n",
    "    # 3. Language\n",
    "    lang = [tweet['lang'] for tweet in json_response['data']]\n",
    "\n",
    "    # 4. Tweet metrics\n",
    "    retweet_count = [tweet['public_metrics']['retweet_count'] for tweet in json['data']]\n",
    "    reply_count = [tweet['public_metrics']['reply_count'] for tweet in json['data']]\n",
    "    like_count = [tweet['public_metrics']['like_count'] for tweet in json['data']]\n",
    "    quote_count = [tweet['public_metrics']['quote_count'] for tweet in json['data']]\n",
    "\n",
    "    # 5. Tweet text\n",
    "    text = [tweet['text'] for tweet in json['data']]\n",
    "    \n",
    "    # 6. URL \n",
    "    url = []\n",
    "    for tweet in json['data']:\n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for link in tweet['entities']['urls']:\n",
    "                url.append(link['expanded_url'])\n",
    "        else:\n",
    "            url.append('')\n",
    "    \n",
    "    # Create df and append everything \n",
    "    dataframe = pd.DataFrame(columns=['User', 'Created', 'Language', 'Likes', 'Quotes', 'Replies', 'RTs', 'Tweet', 'URL'])\n",
    "    dataframe['User'] = pd.Series(user).astype('string')\n",
    "    dataframe['Created'] = pd.Series(created_at)\n",
    "    dataframe['Language'] = pd.Series(lang).astype('string')\n",
    "    dataframe['Likes'] = pd.Series(like_count)\n",
    "    dataframe['Quotes'] = pd.Series(quote_count)\n",
    "    dataframe['Replies'] = pd.Series(reply_count)\n",
    "    dataframe['RTs'] = pd.Series(retweet_count)\n",
    "    dataframe['Tweet'] = pd.Series(text).astype('string')\n",
    "    dataframe['URL'] = pd.Series(url).astype('string')   \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and path\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URLs for title and text \n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        try:\n",
    "            page = requests.get(URL)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            if soup and soup.find('head') and soup.find('body') is not None:\n",
    "                title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip()\n",
    "                text = ' '.join([p.text for p in soup.find('body').find_all('p')]).strip()\n",
    "                new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "                links = links.append(new_row, ignore_index=True)\n",
    "        except AssertionError:\n",
    "            pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_research = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/music_research_21.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add = music_research.url.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_9= scrape_links(links_to_add[550:613])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_9.to_excel(path+'scrape9.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_18 = scrape_links(link_list[1800:1850])\n",
    "links_to_add_18.to_csv(path+'scrape18.csv')\n",
    "links_to_add = links_to_add[links_to_add.Description != ''].reset_index(drop=True)\n",
    "#links_to_add.to_csv(path+'test_2.csv')\n",
    "#links_to_add.to_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/DATA/TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_oral_history_link_scrape.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT PREDICTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(t_input, t_feature, target, score_type, filename, path):\n",
    "    count_vect = CountVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_count = count_vect.fit_transform(t_input[t_feature])\n",
    "    x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=2, scoring=score_type)\n",
    "    model.fit(x_train, y_train)\n",
    "    saved_model = f'LOGREG_RELEVANCE/{filename}_model.pkl'\n",
    "    vectorizer = f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+saved_model, 'wb'))\n",
    "    pickle.dump(vectorizer, open(path+vectorizer, 'wb'))\n",
    "\n",
    "def lr_predict(p_input, p_feature, filename, path):\n",
    "    model = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_model.pkl', 'rb'))\n",
    "    vectorizer = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl', 'rb'))\n",
    "    x_new_count = vectorizer.transform(p_input[p_feature])\n",
    "    x_new_train = tfidf_transformer.transform(x_new_count)\n",
    "    y_predict = model.predict(x_new_train)\n",
    "    scores = model.decision_function(x_new_train, y_predict)\n",
    "    probability = model.predict_log_proba(x_new_train)\n",
    "    results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = results\n",
    "    result['Score'] = [s for s in scores]\n",
    "    result['Probability'] = [p for p in probability]\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even.pkl')\n",
    "training_set_even['Target'] = '1'\n",
    "negative_set = pd.read_csv(path+'LOGREG_RELEVANCE/negative_set.csv')\n",
    "negative_set['Target'] = '0'\n",
    "new_training_set = pd.concat([training_set_even, negative_set])\n",
    "new_training_set['Target'] = new_training_set['Target'].astype('int')\n",
    "new_training_set = new_training_set.reset_index(drop=True)\n",
    "new_training_set.to_pickle(path+'new_training_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neg_set = pd.read_excel(path+'LOGREG_RELEVANCE/non_archive_negative_set.xlsx')\n",
    "new_neg_set = new_neg_set.drop_duplicates(subset=['Title'])\n",
    "new_neg_set['Target'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended.pkl')\n",
    "training_set_even['Target'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_set = pd.concat([training_set_even, new_neg_set])\n",
    "new_training_set['Target'] = new_training_set['Target'].astype('int')\n",
    "new_training_set = new_training_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_set.to_pickle(path+'new_training_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Postdoctoral Researcher - Digital Humanities (...</td>\n",
       "      <td>All jobs You cannot apply for this job anymore...</td>\n",
       "      <td>https://www.academictransfer.com/297340/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Nazmus Saquib - Digital Humanities: Reflection</td>\n",
       "      <td>January 25, 2021 On January 27th, the Islamica...</td>\n",
       "      <td>https://www.nsaquib.org/blog/digital-humanitie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Grants-in-Aid - Arte Publico Press</td>\n",
       "      <td>The University of Houston US Latino Digital Hu...</td>\n",
       "      <td>http://ow.ly/NJs050Dg1eA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Winner of the DHMS Prize - The Medieval Academ...</td>\n",
       "      <td>Remember Me \\n 4/21/2022 » 4/24/2022Medieval A...</td>\n",
       "      <td>https://zurl.co/bGFb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2020 USLDH Mellon-Funded Grants-in-Aid Project...</td>\n",
       "      <td>September 8, 2020 The University of Houston US...</td>\n",
       "      <td>https://l8r.it/JPWw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>University of Amsterdam: Henkjan Honing and Sa...</td>\n",
       "      <td>The funding was granted by the Platform Digita...</td>\n",
       "      <td>https://indiaeducationdiary.in/university-of-a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>New Tung Auditorium Affilliate Organisations a...</td>\n",
       "      <td>The University of Liverpool is delighted to an...</td>\n",
       "      <td>https://www.miragenews.com/new-tung-auditorium...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>25 Surprising Remote Jobs You Can Do From Home...</td>\n",
       "      <td>Find a job faster! 50+ job categories Hand-scr...</td>\n",
       "      <td>https://buff.ly/2OCw3WR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>Digitizing the Grauman Collection – Silent Fil...</td>\n",
       "      <td>Silent Film Sound &amp; Music Archive a digital re...</td>\n",
       "      <td>https://www.sfsma.org/ARK/22915/digitizing-the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Music Index with Full Text | EBSCO</td>\n",
       "      <td>Music Index with Full Text covers every aspect...</td>\n",
       "      <td>https://ebsco.is/2ZbtOSp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "544   Postdoctoral Researcher - Digital Humanities (...   \n",
       "545      Nazmus Saquib - Digital Humanities: Reflection   \n",
       "546                  Grants-in-Aid - Arte Publico Press   \n",
       "547   Winner of the DHMS Prize - The Medieval Academ...   \n",
       "548   2020 USLDH Mellon-Funded Grants-in-Aid Project...   \n",
       "...                                                 ...   \n",
       "1065  University of Amsterdam: Henkjan Honing and Sa...   \n",
       "1066  New Tung Auditorium Affilliate Organisations a...   \n",
       "1067  25 Surprising Remote Jobs You Can Do From Home...   \n",
       "1068  Digitizing the Grauman Collection – Silent Fil...   \n",
       "1069                 Music Index with Full Text | EBSCO   \n",
       "\n",
       "                                            Description  \\\n",
       "544   All jobs You cannot apply for this job anymore...   \n",
       "545   January 25, 2021 On January 27th, the Islamica...   \n",
       "546   The University of Houston US Latino Digital Hu...   \n",
       "547   Remember Me \\n 4/21/2022 » 4/24/2022Medieval A...   \n",
       "548   September 8, 2020 The University of Houston US...   \n",
       "...                                                 ...   \n",
       "1065  The funding was granted by the Platform Digita...   \n",
       "1066  The University of Liverpool is delighted to an...   \n",
       "1067  Find a job faster! 50+ job categories Hand-scr...   \n",
       "1068  Silent Film Sound & Music Archive a digital re...   \n",
       "1069  Music Index with Full Text covers every aspect...   \n",
       "\n",
       "                                                    URL  Target  \n",
       "544            https://www.academictransfer.com/297340/       0  \n",
       "545   https://www.nsaquib.org/blog/digital-humanitie...       0  \n",
       "546                            http://ow.ly/NJs050Dg1eA       0  \n",
       "547                                https://zurl.co/bGFb       0  \n",
       "548                                 https://l8r.it/JPWw       0  \n",
       "...                                                 ...     ...  \n",
       "1065  https://indiaeducationdiary.in/university-of-a...       0  \n",
       "1066  https://www.miragenews.com/new-tung-auditorium...       0  \n",
       "1067                            https://buff.ly/2OCw3WR       0  \n",
       "1068  https://www.sfsma.org/ARK/22915/digitizing-the...       0  \n",
       "1069                           https://ebsco.is/2ZbtOSp       0  \n",
       "\n",
       "[526 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_training_set.loc[new_training_set['Target'] == 0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
