{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , json , csv , datetime , dateutil.parser , unicodedata , time\n",
    "from datetime import datetime , date , timedelta \n",
    "# classifier\n",
    "import pandas as pd\n",
    "from pandas import Timestamp as timestamp\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip3 install trafilatura\n",
    "import trafilatura\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "\n",
    "# descriptions training set\n",
    "new_training_set = pd.read_pickle(path+'LOGREG_RELEVANCE/new_training_set.pkl')\n",
    "\n",
    "# negative twitter training set\n",
    "dh = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/digital_humanities_2021.pkl')\n",
    "music_company = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/music_company_2021.pkl')\n",
    "twitter_neg = pd.concat([dh, music_company])\n",
    "twitter_neg = twitter_neg.loc[twitter_neg['lang'] == 'en']\n",
    "twitter_neg['Target'] = '0'\n",
    "twitter_neg = twitter_neg.sample(n=4379, random_state=56)\n",
    "twitter_neg = twitter_neg[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "#positive twitter training set \n",
    "music_collection = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_collection.pkl')\n",
    "song_dataset = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_song_dataset.pkl')\n",
    "sound_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS/twitter_sound_archive.pkl')\n",
    "digital_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_digital_archive.pkl')\n",
    "music_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_archive.pkl')\n",
    "digi_music_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_digital_music_archive.pkl')\n",
    "midi_file = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_midi_file.pkl')\n",
    "music_data = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_data.pkl')\n",
    "music_research = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS/twitter_music_research.pkl')\n",
    "music_dataset = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_dataset.pkl')\n",
    "twitter_pos = pd.concat([sound_archive, music_collection, digital_archive, music_archive, song_dataset, digi_music_archive, midi_file, music_data, music_research, music_dataset])\n",
    "twitter_pos = twitter_pos.loc[twitter_pos['lang'] == 'en']\n",
    "twitter_pos['Target'] = '1'\n",
    "twitter_pos = twitter_pos[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "# create the twitter training set\n",
    "twitter_set = pd.concat([twitter_pos, twitter_neg])\n",
    "twitter_set['Target'] = twitter_set['Target'].astype('int')\n",
    "twitter_set = twitter_set.reset_index(drop=True)\n",
    "\n",
    "discard = ['youtu', '404', 'Not Found', 'bandcamp']\n",
    "\n",
    "# twitter prediction set \n",
    "#prediction_twitter = pd.read_pickle(path+'TWITTER_SEARCHES/PREDICTIONS/digital_archive_22.pkl')\n",
    "#prediction_twitter = prediction_twitter.loc[prediction_twitter['lang'] == 'en']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_training(t_input, t_feature, target, cv_int, score_type, filename, path):\n",
    "    \"\"\" Create a text classifier based on Logistic regression and TF-IDF. Use cross validation \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t_input: list\n",
    "        dataframe including the training set\n",
    "    t_feature: list\n",
    "        df column, text of tweet or description of the resource\n",
    "    target: list\n",
    "        df column, [0,1] values\n",
    "    cv_int: int\n",
    "        the number of cross validation folding\n",
    "    score_type: str\n",
    "        precision or recall\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    \"\"\"\n",
    "    # TODO eda to define max_features=1000\n",
    "      \n",
    "    #count_vect = CountVectorizer()\n",
    "    #tfidf_transformer = TfidfTransformer() \n",
    "    #x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    tfidf_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=1000) \n",
    "    x_train = tfidf_transformer.fit_transform(t_input[t_feature])\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=cv_int, scoring=score_type)\n",
    "    \n",
    "    # export\n",
    "    model.fit(x_train, y_train)\n",
    "    export_model = f'MODELS/{filename}_model.pkl'\n",
    "    export_vectorizer = f'MODELS/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+export_model, 'wb'))\n",
    "    pickle.dump(tfidf_transformer, open(path+export_vectorizer, 'wb'))\n",
    "    \n",
    "    # report\n",
    "    y_pred = cross_val_predict(model, x_train, y_train, cv=cv_int)\n",
    "    report = classification_report(y_train, y_pred)\n",
    "    print('report:', report, sep='\\n')\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def lr_predict(path, filename, p_input, p_feature):\n",
    "    \"\"\" Classify text using a pickled model based on Logistic regression and TF-IDF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_input: list\n",
    "        dataframe including the prediction set\n",
    "    p_feature: list\n",
    "        df column, text of tweet or description of the resource\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    \"\"\"\n",
    "    export_model = f'{path}MODELS/{filename}_model.pkl'\n",
    "    export_vectorizer = f'{path}MODELS/{filename}_vectorizer.pkl'\n",
    "    model = pickle.load(open(export_model, 'rb'))\n",
    "    tfidf_transformer = pickle.load(open(export_vectorizer, 'rb'))\n",
    "  \n",
    "    #result = loaded_model.score(X_test, Y_test)\n",
    "    #x_new_count = count_vect.transform(p_input[p_feature])\n",
    "    x_predict = tfidf_transformer.transform(p_input[p_feature])\n",
    "    y_predict = model.predict(x_predict)\n",
    "    scores = model.decision_function(x_predict)\n",
    "    probability = model.predict_proba(x_predict)\n",
    "    \n",
    "    #results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = y_predict\n",
    "    result['Score'] = scores\n",
    "    result['Probability'] = probability[:,1]\n",
    "    result['Input Length'] = result[p_feature].str.len()\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results):\n",
    "        search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "        #change params based on the endpoint you are using\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return (search_url, query_params)\n",
    "    \n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    " \n",
    "def append_to_csv(json_response, fileName):\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    #setup usernames via includes\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        # 1. Username\n",
    "        author_id = tweet['author_id']\n",
    "        user = username[author_id]\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 4. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        #5. URLs \n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for url in tweet['entities']['urls']:\n",
    "                url = [url['expanded_url'] for url in tweet['entities']['urls'] if 'twitter.com' not in url['expanded_url']]\n",
    "                url = ', '.join(url)\n",
    "        else:\n",
    "            url = \" \"\n",
    "        \n",
    "        #6. Tweet text\n",
    "        text = tweet['text'] \n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [user, created_at, lang, like_count, quote_count, reply_count, retweet_count, text, url]\n",
    "\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1    \n",
    "    \n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "    \n",
    "def twitter_search(token, input_keywords, start, end, mresults, mcount, path='../'):\n",
    "    \n",
    "    #TODO filter tweets in english only OR tweak TF-IDF stopwords (lang detection)\n",
    "    #TODO clean tweets from @ and emoji\n",
    "    bearer_token = token\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    input_keywords   \n",
    "    start_list = start\n",
    "    end_list =  end\n",
    "    max_results = mresults\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    file_name = str(end[0]).replace(':','-').replace('/','-')\n",
    "    csvFile = open(f'{path}TWITTER_SEARCHES/{file_name}.csv', \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['user', 'created_at', 'lang', 'like_count', 'quote_count', 'reply_count','retweet_count','tweet', 'url'])\n",
    "    csvFile.close()\n",
    "\n",
    "    for i in range(0,len(start_list)):\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = mcount # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            for keyword in input_keywords:\n",
    "                url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "                json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                    # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        append_to_csv(json_response, f'{path}TWITTER_SEARCHES/{file_name}.csv')\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(5)                \n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count is not None and result_count > 0:\n",
    "                        print(\"-------------------\")\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        append_to_csv(json_response, f'{path}TWITTER_SEARCHES/{file_name}.csv')\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(5)\n",
    "\n",
    "                    #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                time.sleep(5)\n",
    "    print(\"Total number of results: \", total_tweets)\n",
    "    \n",
    "    df = pd.read_csv(f'{path}TWITTER_SEARCHES/{file_name}.csv', keep_default_na=False, dtype={\"user\": \"string\", \"lang\": \"string\", \"tweet\": \"string\", \"url\": \"string\"})\n",
    "    \n",
    "    #clean the tweet from meentions and hashtags\n",
    "    df['tweet'].replace( { r\"@[A-Za-z0-9_]+\" : '' }, inplace= True, regex = True)\n",
    "    df['tweet'].replace( { r\"#[A-Za-z0-9_]+\" : '' }, inplace= True, regex = True)\n",
    "    \n",
    "    #remove tweets that are not in english\n",
    "    df = df[df['lang'].isin(['en'])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    \n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        page = None\n",
    "        ARTICLE = ''\n",
    "        try:\n",
    "            x = requests.head(URL)\n",
    "            content_type = x.headers[\"Content-Type\"] if \"Content-Type\" in x.headers else \"None\"\n",
    "            if (\"text/html\" in content_type.lower()):\n",
    "                page = requests.get(URL)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if page:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip() \\\n",
    "                if soup and soup.find('head') and soup.find('body') is not None \\\n",
    "                else URL\n",
    "            \n",
    "            try:\n",
    "                downloaded = trafilatura.fetch_url(URL)\n",
    "                ARTICLE = trafilatura.extract(downloaded, include_comments=False, include_tables=True)\n",
    "            except Exception:\n",
    "                results = soup.find_all(['h1', 'p'])\n",
    "                text = [result.text for result in results]\n",
    "                ARTICLE = ' '.join(text)\n",
    "            \n",
    "            if len(ARTICLE) > 200:\n",
    "                # text summarisation\n",
    "                max_chunk = 500\n",
    "                #removing special characters and replacing with end of sentence\n",
    "                ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
    "                ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
    "                ARTICLE = ARTICLE.replace('!', '!<eos>')\n",
    "                sentences = ARTICLE.split('<eos>')\n",
    "                current_chunk = 0 \n",
    "                chunks = []\n",
    "\n",
    "                # split text to process\n",
    "                for sentence in sentences:\n",
    "                    if len(chunks) == current_chunk + 1: \n",
    "                        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                            chunks[current_chunk].extend(sentence.split(' '))\n",
    "                        else:\n",
    "                            current_chunk += 1\n",
    "                            chunks.append(sentence.split(' '))\n",
    "                    else:\n",
    "                        chunks.append(sentence.split(' '))\n",
    "\n",
    "                for chunk_id in range(len(chunks)):\n",
    "                    chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "                try:\n",
    "                    res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)\n",
    "                    # summary\n",
    "                    text = ' '.join([summ['summary_text'] for summ in res])\n",
    "                except Exception:\n",
    "                    text = ARTICLE\n",
    "                    continue\n",
    "            else:\n",
    "                text = ARTICLE\n",
    "            print(URL,title,'\\n',text)\n",
    "            new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "            links = links.append(new_row, ignore_index=True)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training twitter and descriptions classifiers\n",
    "\n",
    "This is a ONE TIME operation. The models are pickled and loaded later to predict new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time training on twitter\n",
    "#twitter_training_model = lr_training(twitter_set, 'tweet', 'Target', 10, 'precision', 'twitter', path)\n",
    "\n",
    "# one time training on resources\n",
    "#resource_training_model = lr_training(new_training_set, 'Description', 'Target', 10, 'f1','resources',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Twitter\n",
    "\n",
    "Calls Twitter API with the list of keywords and returns the table `prediction_twitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnjx11wuh5yrj4mkjhuskniry4d\n",
      "Start Date:  2022-04-14T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  50\n",
      "-------------------\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnjuwx1rbuwtv4ohbsz51iex3lp\n",
      "Start Date:  2022-04-14T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  100\n",
      "-------------------\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnjuwm9ao7suyc244p1halwqc1p\n",
      "Start Date:  2022-04-14T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  150\n",
      "-------------------\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnjuv4nazwg1z55j6yw2fctuohp\n",
      "Start Date:  2022-04-14T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  200\n",
      "-------------------\n",
      "Total number of results:  200\n"
     ]
    }
   ],
   "source": [
    "token = 'AAAAAAAAAAAAAAAAAAAAAJgsNAEAAAAAQcsgbUnOJJmqmU483%2F8x6n9V1i8%3Df0qaEo9cV1sWP4eyNQ6E9s8BiRjvFTSN9mSqithe8uIXSNP68x'\n",
    "\n",
    "# a selection of keywords from KEYWORDS/bg_summary.csv\n",
    "# keywords = ['sheet music','music archive','music collection','music library','black music','sound recording','midi file','early music','sound archive','music information','music history','music research','musical score','song dataset','library music','music oral','score collection','digitized score']\n",
    "keywords = ['sheet music','music archive','music collection','music library']\n",
    "input_keywords = [k+\" -is:retweet\" for k in keywords] \n",
    "\n",
    "today = date.today()\n",
    "week_ago = today - timedelta(days=7)\n",
    "start = [week_ago.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")]\n",
    "end = [today.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")]\n",
    "\n",
    "mresults = 50 # for each keyword\n",
    "mcount = 50 # for each timespan (only one, last week, here)\n",
    "path='../'\n",
    "\n",
    "prediction_twitter = twitter_search(token, input_keywords, start, end, mresults, mcount, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>music archive</td>\n",
       "      <td>1</td>\n",
       "      <td>19.364708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The \"CD-sized\" order of magnitude encompasses ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.494783</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>271</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you  for the lovely chat around sound, m...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.259046</td>\n",
       "      <td>0.999297</td>\n",
       "      <td>123</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indeed, but music collection &amp;amp; app integ...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.236319</td>\n",
       "      <td>0.999281</td>\n",
       "      <td>87</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>达Time Fountain达\n",
       "Collection  0.2 \n",
       " Play w...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.853723</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>180</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Jane Carter Named President Of Universal Produ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.254717</td>\n",
       "      <td>0.563337</td>\n",
       "      <td>87</td>\n",
       "      <td>https://www.allaccess.com/story/217590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>The potent duo of French/British rapper AVC, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165759</td>\n",
       "      <td>0.541345</td>\n",
       "      <td>281</td>\n",
       "      <td>http://oculate.uk/archive/music/avc-malchiodi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Beatport &amp;amp; Microsoft Surface Announce Educ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125950</td>\n",
       "      <td>0.531446</td>\n",
       "      <td>127</td>\n",
       "      <td>http://dlvr.it/SNx0fZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>What a great collaboration between Music and L...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.124141</td>\n",
       "      <td>0.530996</td>\n",
       "      <td>81</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Jane Carter Named President Of Universal Produ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>0.502010</td>\n",
       "      <td>111</td>\n",
       "      <td>http://dlvr.it/SNwqFD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  Prediction      Score  \\\n",
       "0                                        music archive           1  19.364708   \n",
       "1    The \"CD-sized\" order of magnitude encompasses ...           1  10.494783   \n",
       "2    Thank you  for the lovely chat around sound, m...           1   7.259046   \n",
       "3      Indeed, but music collection &amp; app integ...           1   7.236319   \n",
       "4     达Time Fountain达\n",
       "Collection  0.2 \n",
       " Play w...           1   6.853723   \n",
       "..                                                 ...         ...        ...   \n",
       "157  Jane Carter Named President Of Universal Produ...           1   0.254717   \n",
       "158  The potent duo of French/British rapper AVC, a...           1   0.165759   \n",
       "159  Beatport &amp; Microsoft Surface Announce Educ...           1   0.125950   \n",
       "160  What a great collaboration between Music and L...           1   0.124141   \n",
       "161  Jane Carter Named President Of Universal Produ...           1   0.008041   \n",
       "\n",
       "     Probability  Input Length  \\\n",
       "0       1.000000            13   \n",
       "1       0.999972           271   \n",
       "2       0.999297           123   \n",
       "3       0.999281            87   \n",
       "4       0.998946           180   \n",
       "..           ...           ...   \n",
       "157     0.563337            87   \n",
       "158     0.541345           281   \n",
       "159     0.531446           127   \n",
       "160     0.530996            81   \n",
       "161     0.502010           111   \n",
       "\n",
       "                                                   url  \n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "..                                                 ...  \n",
       "157             https://www.allaccess.com/story/217590  \n",
       "158  http://oculate.uk/archive/music/avc-malchiodi-...  \n",
       "159                              http://dlvr.it/SNx0fZ  \n",
       "160                                                     \n",
       "161                              http://dlvr.it/SNwqFD  \n",
       "\n",
       "[162 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions\n",
    "twitter_predictions = lr_predict(path, 'twitter', prediction_twitter, 'tweet')\n",
    "\n",
    "tweet_predict_cv_df = twitter_predictions.copy().drop_duplicates()\n",
    "tweet_predict_cv_df = tweet_predict_cv_df.loc[tweet_predict_cv_df['Prediction'] == 1]\n",
    "tweet_predict_cv_df = tweet_predict_cv_df[~tweet_predict_cv_df.url.str.contains('|'.join(discard))]\n",
    "tweet_predict_cv_df = tweet_predict_cv_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "tweet_predict_cv_df = tweet_predict_cv_df[['tweet', 'Prediction', 'Score', 'Probability', 'Input Length', 'url']]\n",
    "tweet_predict_cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tinyurl.com/jazz-journals Journals \n",
      "  The National Jazz Archive journal collection is unique and of international importance . It consists of original source material which tells the story of jazz as it happened in a way that no other medium can .\n",
      "https://www.cca-glasgow.com/programme/third-eye-tv Third Eye TV | CCA Glasgow \n",
      "  New music, sound poetry, archival video, interviews & more in celebration of Glasgow's legendary Third Eye Centre . Streaming live Fri-Sat 22-23 Oct and Fri- Sat 29-30 Oct on this page .\n"
     ]
    }
   ],
   "source": [
    "# get links from positive tweets results\n",
    "twitter_link_list = [link for link in tweet_predict_cv_df['url'] if 'twitter' not in link]\n",
    "\n",
    "# scrape URL list\n",
    "links_to_add = scrape_links(twitter_link_list)\n",
    "\n",
    "# remove empty descriptions \n",
    "links_to_add = links_to_add[links_to_add.Description != ''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify web resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Third Eye TV | CCA Glasgow</td>\n",
       "      <td>New music, sound poetry, archival video, inte...</td>\n",
       "      <td>https://www.cca-glasgow.com/programme/third-ey...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.886536</td>\n",
       "      <td>0.979896</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journals</td>\n",
       "      <td>The National Jazz Archive journal collection ...</td>\n",
       "      <td>https://tinyurl.com/jazz-journals</td>\n",
       "      <td>1</td>\n",
       "      <td>2.931182</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title  \\\n",
       "0  Third Eye TV | CCA Glasgow   \n",
       "1                    Journals   \n",
       "\n",
       "                                         Description  \\\n",
       "0   New music, sound poetry, archival video, inte...   \n",
       "1   The National Jazz Archive journal collection ...   \n",
       "\n",
       "                                                 URL  Prediction     Score  \\\n",
       "0  https://www.cca-glasgow.com/programme/third-ey...           1  3.886536   \n",
       "1                  https://tinyurl.com/jazz-journals           1  2.931182   \n",
       "\n",
       "   Probability  Input Length  \n",
       "0     0.979896           187  \n",
       "1     0.949367           210  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources_predictions = lr_predict(path, 'resources', links_to_add, 'Description')\n",
    "\n",
    "resources_preds_cv_df = resources_predictions.copy()\n",
    "resources_preds_cv_df = resources_preds_cv_df.loc[resources_preds_cv_df['Prediction'] == 1]\n",
    "resources_preds_cv_df = resources_preds_cv_df[~resources_preds_cv_df.Title.str.contains('|'.join(discard))]\n",
    "resources_preds_cv_df = resources_preds_cv_df[~resources_preds_cv_df.URL.str.contains('|'.join(discard))]\n",
    "resources_preds_cv_df.sort_values(by='Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
