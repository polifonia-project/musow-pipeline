{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ ME\n",
    "- this notebook is used to test various things while working on the pipeline and save unfinished snippets of code for potential future use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe function\n",
    "\n",
    "def append_to_pd(json):\n",
    "    \n",
    "    #username setup\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #1. username\n",
    "    author_id = [tweet['author_id'] for tweet in json['data']]\n",
    "    user = [username[id] for id in author_id]\n",
    "\n",
    "    # 2. Time created\n",
    "    created_at = [dateutil.parser.parse(tweet['created_at']) for tweet in json['data']]\n",
    "\n",
    "    # 3. Language\n",
    "    lang = [tweet['lang'] for tweet in json_response['data']]\n",
    "\n",
    "    # 4. Tweet metrics\n",
    "    retweet_count = [tweet['public_metrics']['retweet_count'] for tweet in json['data']]\n",
    "    reply_count = [tweet['public_metrics']['reply_count'] for tweet in json['data']]\n",
    "    like_count = [tweet['public_metrics']['like_count'] for tweet in json['data']]\n",
    "    quote_count = [tweet['public_metrics']['quote_count'] for tweet in json['data']]\n",
    "\n",
    "    # 5. Tweet text\n",
    "    text = [tweet['text'] for tweet in json['data']]\n",
    "    \n",
    "    # 6. URL \n",
    "    url = []\n",
    "    for tweet in json['data']:\n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for link in tweet['entities']['urls']:\n",
    "                url.append(link['expanded_url'])\n",
    "        else:\n",
    "            url.append('')\n",
    "    \n",
    "    # Create df and append everything \n",
    "    dataframe = pd.DataFrame(columns=['User', 'Created', 'Language', 'Likes', 'Quotes', 'Replies', 'RTs', 'Tweet', 'URL'])\n",
    "    dataframe['User'] = pd.Series(user).astype('string')\n",
    "    dataframe['Created'] = pd.Series(created_at)\n",
    "    dataframe['Language'] = pd.Series(lang).astype('string')\n",
    "    dataframe['Likes'] = pd.Series(like_count)\n",
    "    dataframe['Quotes'] = pd.Series(quote_count)\n",
    "    dataframe['Replies'] = pd.Series(reply_count)\n",
    "    dataframe['RTs'] = pd.Series(retweet_count)\n",
    "    dataframe['Tweet'] = pd.Series(text).astype('string')\n",
    "    dataframe['URL'] = pd.Series(url).astype('string')   \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and path\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URLs for title and text \n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        try:\n",
    "            page = requests.get(URL)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            if soup and soup.find('head') and soup.find('body') is not None:\n",
    "                title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip()\n",
    "                text = ' '.join([p.text for p in soup.find('body').find_all('p')]).strip()\n",
    "                new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "                links = links.append(new_row, ignore_index=True)\n",
    "        except AssertionError:\n",
    "            pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_research = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/music_research_21.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add = music_research.url.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_9= scrape_links(links_to_add[550:613])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_9.to_excel(path+'scrape9.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_18 = scrape_links(link_list[1800:1850])\n",
    "links_to_add_18.to_csv(path+'scrape18.csv')\n",
    "links_to_add = links_to_add[links_to_add.Description != ''].reset_index(drop=True)\n",
    "#links_to_add.to_csv(path+'test_2.csv')\n",
    "#links_to_add.to_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/DATA/TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_oral_history_link_scrape.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT PREDICTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(t_input, t_feature, target, score_type, filename, path):\n",
    "    count_vect = CountVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_count = count_vect.fit_transform(t_input[t_feature])\n",
    "    x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=2, scoring=score_type)\n",
    "    model.fit(x_train, y_train)\n",
    "    saved_model = f'LOGREG_RELEVANCE/{filename}_model.pkl'\n",
    "    vectorizer = f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+saved_model, 'wb'))\n",
    "    pickle.dump(vectorizer, open(path+vectorizer, 'wb'))\n",
    "\n",
    "def lr_predict(p_input, p_feature, filename, path):\n",
    "    model = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_model.pkl', 'rb'))\n",
    "    vectorizer = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl', 'rb'))\n",
    "    x_new_count = vectorizer.transform(p_input[p_feature])\n",
    "    x_new_train = tfidf_transformer.transform(x_new_count)\n",
    "    y_predict = model.predict(x_new_train)\n",
    "    scores = model.decision_function(x_new_train, y_predict)\n",
    "    probability = model.predict_log_proba(x_new_train)\n",
    "    results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = results\n",
    "    result['Score'] = [s for s in scores]\n",
    "    result['Probability'] = [p for p in probability]\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even.pkl')\n",
    "training_set_even['Target'] = '1'\n",
    "negative_set = pd.read_csv(path+'LOGREG_RELEVANCE/negative_set.csv')\n",
    "negative_set['Target'] = '0'\n",
    "new_training_set = pd.concat([training_set_even, negative_set])\n",
    "new_training_set['Target'] = new_training_set['Target'].astype('int')\n",
    "new_training_set = new_training_set.reset_index(drop=True)\n",
    "new_training_set.to_pickle(path+'new_training_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neg_set = pd.read_excel(path+'LOGREG_RELEVANCE/non_archive_negative_set.xlsx')\n",
    "new_neg_set = new_neg_set.drop_duplicates(subset=['Title'])\n",
    "new_neg_set['Target'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended.pkl')\n",
    "training_set_even['Target'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_set = pd.concat([training_set_even, new_neg_set])\n",
    "new_training_set['Target'] = new_training_set['Target'].astype('int')\n",
    "new_training_set = new_training_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_set.to_pickle(path+'new_training_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Postdoctoral Researcher - Digital Humanities (...</td>\n",
       "      <td>All jobs You cannot apply for this job anymore...</td>\n",
       "      <td>https://www.academictransfer.com/297340/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Nazmus Saquib - Digital Humanities: Reflection</td>\n",
       "      <td>January 25, 2021 On January 27th, the Islamica...</td>\n",
       "      <td>https://www.nsaquib.org/blog/digital-humanitie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Grants-in-Aid - Arte Publico Press</td>\n",
       "      <td>The University of Houston US Latino Digital Hu...</td>\n",
       "      <td>http://ow.ly/NJs050Dg1eA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Winner of the DHMS Prize - The Medieval Academ...</td>\n",
       "      <td>Remember Me \\n 4/21/2022 » 4/24/2022Medieval A...</td>\n",
       "      <td>https://zurl.co/bGFb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2020 USLDH Mellon-Funded Grants-in-Aid Project...</td>\n",
       "      <td>September 8, 2020 The University of Houston US...</td>\n",
       "      <td>https://l8r.it/JPWw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>University of Amsterdam: Henkjan Honing and Sa...</td>\n",
       "      <td>The funding was granted by the Platform Digita...</td>\n",
       "      <td>https://indiaeducationdiary.in/university-of-a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>New Tung Auditorium Affilliate Organisations a...</td>\n",
       "      <td>The University of Liverpool is delighted to an...</td>\n",
       "      <td>https://www.miragenews.com/new-tung-auditorium...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>25 Surprising Remote Jobs You Can Do From Home...</td>\n",
       "      <td>Find a job faster! 50+ job categories Hand-scr...</td>\n",
       "      <td>https://buff.ly/2OCw3WR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>Digitizing the Grauman Collection – Silent Fil...</td>\n",
       "      <td>Silent Film Sound &amp; Music Archive a digital re...</td>\n",
       "      <td>https://www.sfsma.org/ARK/22915/digitizing-the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Music Index with Full Text | EBSCO</td>\n",
       "      <td>Music Index with Full Text covers every aspect...</td>\n",
       "      <td>https://ebsco.is/2ZbtOSp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "544   Postdoctoral Researcher - Digital Humanities (...   \n",
       "545      Nazmus Saquib - Digital Humanities: Reflection   \n",
       "546                  Grants-in-Aid - Arte Publico Press   \n",
       "547   Winner of the DHMS Prize - The Medieval Academ...   \n",
       "548   2020 USLDH Mellon-Funded Grants-in-Aid Project...   \n",
       "...                                                 ...   \n",
       "1065  University of Amsterdam: Henkjan Honing and Sa...   \n",
       "1066  New Tung Auditorium Affilliate Organisations a...   \n",
       "1067  25 Surprising Remote Jobs You Can Do From Home...   \n",
       "1068  Digitizing the Grauman Collection – Silent Fil...   \n",
       "1069                 Music Index with Full Text | EBSCO   \n",
       "\n",
       "                                            Description  \\\n",
       "544   All jobs You cannot apply for this job anymore...   \n",
       "545   January 25, 2021 On January 27th, the Islamica...   \n",
       "546   The University of Houston US Latino Digital Hu...   \n",
       "547   Remember Me \\n 4/21/2022 » 4/24/2022Medieval A...   \n",
       "548   September 8, 2020 The University of Houston US...   \n",
       "...                                                 ...   \n",
       "1065  The funding was granted by the Platform Digita...   \n",
       "1066  The University of Liverpool is delighted to an...   \n",
       "1067  Find a job faster! 50+ job categories Hand-scr...   \n",
       "1068  Silent Film Sound & Music Archive a digital re...   \n",
       "1069  Music Index with Full Text covers every aspect...   \n",
       "\n",
       "                                                    URL  Target  \n",
       "544            https://www.academictransfer.com/297340/       0  \n",
       "545   https://www.nsaquib.org/blog/digital-humanitie...       0  \n",
       "546                            http://ow.ly/NJs050Dg1eA       0  \n",
       "547                                https://zurl.co/bGFb       0  \n",
       "548                                 https://l8r.it/JPWw       0  \n",
       "...                                                 ...     ...  \n",
       "1065  https://indiaeducationdiary.in/university-of-a...       0  \n",
       "1066  https://www.miragenews.com/new-tung-auditorium...       0  \n",
       "1067                            https://buff.ly/2OCw3WR       0  \n",
       "1068  https://www.sfsma.org/ARK/22915/digitizing-the...       0  \n",
       "1069                           https://ebsco.is/2ZbtOSp       0  \n",
       "\n",
       "[526 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_training_set.loc[new_training_set['Target'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_desc(text):\n",
    "    summarizer = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
    "    # text summarisation\n",
    "    max_chunk = 500\n",
    "    #removing special characters and replacing with end of sentence\n",
    "    text = text.replace('.', '.<eos>')\n",
    "    text = text.replace('?', '?<eos>')\n",
    "    text = text.replace('!', '!<eos>')\n",
    "    sentences = text.split('<eos>')\n",
    "    current_chunk = 0 \n",
    "    chunks = []\n",
    "\n",
    "    # split text to process\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1: \n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            chunks.append(sentence.split(' '))\n",
    "\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "        res = summarizer(chunks, min_length = int(0.2 * len(text)), max_length = int(0.5 * len(text)), do_sample=False)\n",
    "        # summary\n",
    "        new_text = ' '.join([summ['summary_text'] for summ in res])\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_desc_training_v2 = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/archive_desc_training_v2.pkl')\n",
    "archive_desc_training_v2 = archive_desc_training_v2.loc[archive_desc_training_v2['Target'] == 1]\n",
    "descs = [d for d in archive_desc_training_v2['Description']]\n",
    "len(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs_summed = []\n",
    "for d in descs:\n",
    "    d = summarize_desc(d)\n",
    "    descs_summed.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Delia Derbyshire was a composer of early electronic music in the UK . This archive contains 267 audio tapes that have been digitized and are available to listen to at the library . The tapes are largely related to Derbysire’s freelance projects as most of her work for the BBC is kept in the BBC Archive Centre at Perivale . There are also film donations from Madelon Hooykaas, an audio interview with Delia from Jo Hutton and a 90 second demo cue for an unmade film from the early 1980s donated by the visual artist Elisabeth Kozmian . Physical items include books (about music and electronic music), working notes, correspondence and letters referring to Delia's work, sound cue sheets, interview transcriptions and newspaper cuttings as well as childhood school notebooks and drawings . The physical items include school notebooks, pencils and pencils, notebooks and notebooks and photographs . The archive is open to the public to download and listen to all of the audio tapes from the library. The audio tapes are available for purchase by the library at £1,800-a-year-old. For more information, visit www.bbc.org.uk.uk/diary/delia.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_desc('Delia Derbyshire was a composer of early electronic music in the UK. This archive contains 267 audio tapes that have been digitized and are available to listen to at the library, including make-up tapes (uncovering the process of how Delia made her music) and recordings from radio and albums by the likes of Art Blakey, Karlheinz Stockhausen, Yusef Lateef and Can. The tapes are largely related to Derbyshire’s freelance projects as most of her work for the BBC is kept in the BBC Archive Centre at Perivale. There are also film donations from Madelon Hooykaas, an audio interview with Delia from Jo Hutton and a 90 second demo cue for an unmade film from the early 1980s donated by the visual artist Elisabeth Kozmian. The physical items include books (about music and electronic music), working notes, correspondence and letters referring to Delia’s work, sound cue sheets, interview transcriptions and newspaper cuttings as well as childhood school notebooks and drawings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
