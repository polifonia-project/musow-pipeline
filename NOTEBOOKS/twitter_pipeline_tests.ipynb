{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "- This notebook is used to test various aspects of the twitter pipeline \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "import csv , dateutil.parser , time\n",
    "from datetime import date , timedelta \n",
    "import os\n",
    "# classifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip3 install trafilatura\n",
    "import trafilatura\n",
    "from transformers import pipeline\n",
    "#cleaning \n",
    "import emoji\n",
    "import re\n",
    "#functions\n",
    "from PYTHON_FILES.LogReg_Searches import LogRegSearches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions training set -> v2 = musow+mji descriptions vs summarized scrapes from twitter searches  \n",
    "archive_desc_training_v2 = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/archive_desc_training_v2.pkl')\n",
    "\n",
    "# twitter training set -> v1 = tweets from bigrams vs tweets for digital humanities and music company \n",
    "twitter_training_set_v1 = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/twitter_training_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LevittDF</td>\n",
       "      <td>2021-01-30 23:23:32+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Check out \"Didn't We\" BY JiMMY WEBB sheet musi...</td>\n",
       "      <td>https://www.ebay.com/itm/284166722848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DXSteveX</td>\n",
       "      <td>2021-01-30 23:01:58+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Memories - Xenoblade Chronicles (Piano Cover, ...</td>\n",
       "      <td>https://youtu.be/unX1F7D2OBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CathyGriffindor</td>\n",
       "      <td>2021-01-30 22:44:34+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EPUB Free The Beatles Sheet Music Collection =...</td>\n",
       "      <td>https://redjourneylibrary.blogspot.com/book55....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HarmonyTabs</td>\n",
       "      <td>2021-01-30 22:28:54+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I Want A Hippopotamus For Christmas (Hippo The...</td>\n",
       "      <td>https://is.gd/YdFViV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LevittDF</td>\n",
       "      <td>2021-01-30 22:22:01+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Check out Old Days Sheet Music https://t.co/IW...</td>\n",
       "      <td>https://www.ebay.com/itm/284166732838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11654</th>\n",
       "      <td>LibrarySheet</td>\n",
       "      <td>2021-12-26 18:34:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Teachers and Mentors. Students and Teachers . ...</td>\n",
       "      <td>https://sheetmusiclibrary.website/2021/01/16/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11663</th>\n",
       "      <td>reading_your</td>\n",
       "      <td>2021-12-26 17:03:56+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>list book online new all book free | DANDY : 2...</td>\n",
       "      <td>http://goo.gl/RpFbdl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11665</th>\n",
       "      <td>decacorde</td>\n",
       "      <td>2021-12-26 16:53:40+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>“Ghrostmas Ghosts” my new 10 string guitar ori...</td>\n",
       "      <td>https://www.patreon.com/posts/60346043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11669</th>\n",
       "      <td>djtbird1</td>\n",
       "      <td>2021-12-26 16:49:53+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#CulturalLesson 483: \"Letto\" is #Italian for b...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ghetto/, https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11670</th>\n",
       "      <td>jokottenweb</td>\n",
       "      <td>2021-12-26 16:47:30+00:00</td>\n",
       "      <td>de</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gariboldi G.Twenty Small Case Studies -flöte 1...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4116 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user                 created_at lang  like_count  \\\n",
       "3             LevittDF  2021-01-30 23:23:32+00:00   en           0   \n",
       "5             DXSteveX  2021-01-30 23:01:58+00:00   en           2   \n",
       "7      CathyGriffindor  2021-01-30 22:44:34+00:00   en           0   \n",
       "11         HarmonyTabs  2021-01-30 22:28:54+00:00   en           0   \n",
       "12            LevittDF  2021-01-30 22:22:01+00:00   en           0   \n",
       "...                ...                        ...  ...         ...   \n",
       "11654     LibrarySheet  2021-12-26 18:34:59+00:00   en           3   \n",
       "11663     reading_your  2021-12-26 17:03:56+00:00   en           0   \n",
       "11665        decacorde  2021-12-26 16:53:40+00:00   en           8   \n",
       "11669         djtbird1  2021-12-26 16:49:53+00:00   en           1   \n",
       "11670      jokottenweb  2021-12-26 16:47:30+00:00   de           0   \n",
       "\n",
       "       quote_count  reply_count  retweet_count  \\\n",
       "3                0            0              0   \n",
       "5                0            0              1   \n",
       "7                0            0              0   \n",
       "11               0            0              0   \n",
       "12               0            0              0   \n",
       "...            ...          ...            ...   \n",
       "11654            0            0              0   \n",
       "11663            0            0              0   \n",
       "11665            0            1              0   \n",
       "11669            0            0              0   \n",
       "11670            0            0              0   \n",
       "\n",
       "                                                   tweet  \\\n",
       "3      Check out \"Didn't We\" BY JiMMY WEBB sheet musi...   \n",
       "5      Memories - Xenoblade Chronicles (Piano Cover, ...   \n",
       "7      EPUB Free The Beatles Sheet Music Collection =...   \n",
       "11     I Want A Hippopotamus For Christmas (Hippo The...   \n",
       "12     Check out Old Days Sheet Music https://t.co/IW...   \n",
       "...                                                  ...   \n",
       "11654  Teachers and Mentors. Students and Teachers . ...   \n",
       "11663  list book online new all book free | DANDY : 2...   \n",
       "11665  “Ghrostmas Ghosts” my new 10 string guitar ori...   \n",
       "11669  #CulturalLesson 483: \"Letto\" is #Italian for b...   \n",
       "11670  Gariboldi G.Twenty Small Case Studies -flöte 1...   \n",
       "\n",
       "                                                     url  \n",
       "3                  https://www.ebay.com/itm/284166722848  \n",
       "5                           https://youtu.be/unX1F7D2OBM  \n",
       "7      https://redjourneylibrary.blogspot.com/book55....  \n",
       "11                                  https://is.gd/YdFViV  \n",
       "12                 https://www.ebay.com/itm/284166732838  \n",
       "...                                                  ...  \n",
       "11654  https://sheetmusiclibrary.website/2021/01/16/t...  \n",
       "11663                               http://goo.gl/RpFbdl  \n",
       "11665             https://www.patreon.com/posts/60346043  \n",
       "11669  https://en.wikipedia.org/wiki/Ghetto/, https:/...  \n",
       "11670                                                     \n",
       "\n",
       "[4116 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_collection = pd.read_pickle(path+'OLD_DO_NOT_USE/MUSOW BIGRAMS/twitter_sheet_music.pkl')\n",
    "#music_collection['tweet'] = music_collection['tweet'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "music_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_collection.to_csv(path+'twitter_music_collection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_collection.to_excel(path+'twitter_sheet_music.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_training(t_input, t_feature, target, cv_int, score_type, filename, path):\n",
    "    \"\"\" Create a text classifier based on Logistic regression and TF-IDF. Use cross validation \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t_input: \n",
    "        dataframe of the training set\n",
    "    t_feature: \n",
    "        df column, text of tweet or description of the resource\n",
    "    target: \n",
    "        df column, [0,1] values\n",
    "    cv_int: int\n",
    "        the number of cross validation folding\n",
    "    score_type: str\n",
    "        precision or recall\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    \"\"\"\n",
    "    # TODO eda to define max_features=1000\n",
    "      \n",
    "    tfidf_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=1000) \n",
    "    x_train = tfidf_transformer.fit_transform(t_input[t_feature])\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=cv_int, scoring=score_type)\n",
    "    \n",
    "    # export\n",
    "    model.fit(x_train, y_train)\n",
    "    export_model = f'LOGREG_RELEVANCE/MODELS/{filename}_model.pkl'\n",
    "    export_vectorizer = f'LOGREG_RELEVANCE/MODELS/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+export_model, 'wb'))\n",
    "    pickle.dump(tfidf_transformer, open(path+export_vectorizer, 'wb'))\n",
    "    \n",
    "    # report\n",
    "    y_pred = cross_val_predict(model, x_train, y_train, cv=cv_int)\n",
    "    report = classification_report(y_train, y_pred)\n",
    "    print('report:', report, sep='\\n')\n",
    "    return model\n",
    "    \n",
    "def lr_predict(path, filename, p_input, p_feature):\n",
    "    \"\"\" Classify text using a pickled model based on Logistic regression and TF-IDF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_input: \n",
    "        dataframe of the prediction set\n",
    "    p_feature: \n",
    "        df column, text of tweet or description of the resource\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    \"\"\"\n",
    "    export_model = f'{path}LOGREG_RELEVANCE/MODELS/{filename}_model.pkl'\n",
    "    export_vectorizer = f'{path}LOGREG_RELEVANCE/MODELS/{filename}_vectorizer.pkl'\n",
    "    model = pickle.load(open(export_model, 'rb'))\n",
    "    tfidf_transformer = pickle.load(open(export_vectorizer, 'rb'))\n",
    "  \n",
    "    #result = loaded_model.score(X_test, Y_test)\n",
    "    #x_new_count = count_vect.transform(p_input[p_feature])\n",
    "    x_predict = tfidf_transformer.transform(p_input[p_feature])\n",
    "    y_predict = model.predict(x_predict)\n",
    "    scores = model.decision_function(x_predict)\n",
    "    probability = model.predict_proba(x_predict)\n",
    "    \n",
    "    #results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = y_predict\n",
    "    result['Score'] = scores\n",
    "    result['Probability'] = probability[:,1]\n",
    "    result['Input Length'] = result[p_feature].str.len()\n",
    "    return result\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results):\n",
    "        search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "        #change params based on the endpoint you are using\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return (search_url, query_params)\n",
    "    \n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    #setup usernames via includes\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        # 1. Username\n",
    "        author_id = tweet['author_id']\n",
    "        user = username[author_id]\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 4. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        #5. URLs w/ a catch for tweets w/ two links TODO: how to catch more than two links? \n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for url in tweet['entities']['urls']:\n",
    "                url = [url['expanded_url'] for url in tweet['entities']['urls'] if 'twitter.com' not in url['expanded_url']]\n",
    "                url = ', '.join(url)\n",
    "        else:\n",
    "            url = \"\"\n",
    "        \n",
    "        #6. Tweet text\n",
    "        text = give_emoji_free_text(tweet['text']) \n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [user, created_at, lang, like_count, quote_count, reply_count, retweet_count, text, url]\n",
    "\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1    \n",
    "    \n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "\n",
    "def twitter_search(token, keyword, start, end, mresults, mcount, file_name):\n",
    "    \n",
    "    # TODO filter tweets in english only OR tweak TF-IDF stopwords (lang detection)\n",
    "    bearer_token = token\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)} \n",
    "    start_list = start\n",
    "    end_list =  end\n",
    "    max_results = mresults\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    csvFile = open(f'{path}TWITTER_SEARCHES/RAW_SEARCHES/{file_name}.csv', \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['user', 'created_at', 'lang', 'like_count', 'quote_count', 'reply_count','retweet_count','tweet', 'URL'])\n",
    "    csvFile.close()\n",
    "\n",
    "    for i in range(0,len(start_list)):\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = mcount # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, f'{path}TWITTER_SEARCHES/RAW_SEARCHES/{file_name}.csv')\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(f\"Total # of Tweets added for '{keyword}':\", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, f'{path}TWITTER_SEARCHES/RAW_SEARCHES/{file_name}.csv')\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(f\"Total # of Tweets added for '{keyword}':\", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)\n",
    "\n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(5)\n",
    "    print(\"Total number of results:\", total_tweets)\n",
    "    \n",
    "    df = pd.read_csv(f'{path}TWITTER_SEARCHES/RAW_SEARCHES/{file_name}.csv', keep_default_na=False, dtype={\"user\": \"string\", \"lang\": \"string\", \"tweet\": \"string\", \"URL\": \"string\"})\n",
    "    \n",
    "    # clean the tweet from meentions, hashtags, emojis\n",
    "    df['tweet'].replace( { r\"@[A-Za-z0-9_]+\": '' }, inplace= True, regex = True)\n",
    "    df['tweet'].replace( { r\"#\": '' }, inplace= True, regex = True)\n",
    "    \n",
    "    # remove tweets that are not in english, have empty URLs, or have duplicate URLs\n",
    "    df = df[df['lang'].isin(['en'])]\n",
    "    df = df[df.URL != '']\n",
    "    df = df.drop_duplicates(['URL'], keep='last')\n",
    "\n",
    "    #add a column for the search keyword\n",
    "    df['Search KW'] = keyword\n",
    "\n",
    "    #pickle df for reuse\n",
    "    df.to_pickle(f'{path}TWITTER_SEARCHES/RAW_SEARCHES/{file_name}.pkl')\n",
    "\n",
    "def scrape_links(link_list, pred_df, filename):\n",
    "    \"\"\" Scrape links from classified tweets, save scrapes, combine them w/ tweets and return a DF for description classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_input: \n",
    "        dataframe of the prediction set\n",
    "    p_feature: \n",
    "        df column, text of tweet or description of the resource\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    score: int\n",
    "        which prediction score to filter the results by 1/0\n",
    "    discard: variable\n",
    "        a list of terms to check against to remove tweets\n",
    "    filter: str \n",
    "        a string against which to further filter predictions \n",
    "    \"\"\"\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    summarizer = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')\n",
    "    \n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        page = None\n",
    "        ARTICLE = ''\n",
    "        try:\n",
    "            x = requests.head(URL, timeout=15)\n",
    "            content_type = x.headers[\"Content-Type\"] if \"Content-Type\" in x.headers else \"None\"\n",
    "            if (\"text/html\" in content_type.lower()):\n",
    "                page = requests.get(URL, timeout=15)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if page:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip() \\\n",
    "                if soup and soup.find('head') and soup.find('body') is not None \\\n",
    "                else URL\n",
    "            \n",
    "            try:\n",
    "                downloaded = trafilatura.fetch_url(URL)\n",
    "                ARTICLE = trafilatura.extract(downloaded, include_comments=False, include_tables=True, target_language='en', deduplicate=True)\n",
    "            except Exception:\n",
    "                results = soup.find_all(['h1', 'p'])\n",
    "                text = [result.text for result in results]\n",
    "                ARTICLE = ' '.join(text)\n",
    "            \n",
    "            if ARTICLE is not None and len(ARTICLE) > 200:\n",
    "                # text summarisation\n",
    "                max_chunk = 500\n",
    "                #removing special characters and replacing with end of sentence\n",
    "                ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
    "                ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
    "                ARTICLE = ARTICLE.replace('!', '!<eos>')\n",
    "                sentences = ARTICLE.split('<eos>')\n",
    "                current_chunk = 0 \n",
    "                chunks = []\n",
    "\n",
    "                # split text to process\n",
    "                for sentence in sentences:\n",
    "                    if len(chunks) == current_chunk + 1: \n",
    "                        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                            chunks[current_chunk].extend(sentence.split(' '))\n",
    "                        else:\n",
    "                            current_chunk += 1\n",
    "                            chunks.append(sentence.split(' '))\n",
    "                    else:\n",
    "                        chunks.append(sentence.split(' '))\n",
    "\n",
    "                for chunk_id in range(len(chunks)):\n",
    "                    chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "                try:\n",
    "                    res = summarizer(chunks, min_length = int(0.2 * len(text)), max_length = int(0.5 * len(text)), do_sample=False)\n",
    "                    # summary\n",
    "                    text = ' '.join([summ['summary_text'] for summ in res])\n",
    "                except Exception:\n",
    "                    text = ARTICLE\n",
    "                    continue\n",
    "            else:\n",
    "                text = ARTICLE\n",
    "            print(URL)\n",
    "            new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "            new_df = pd.DataFrame(data=new_row, index=[0])\n",
    "            links = pd.concat([links, new_df], ignore_index=True)\n",
    "    discard = ['None', '! D O C T Y P E h t m l >', '! d o c t y p e h t m l >', '! D O C T Y P E H T M L >']\n",
    "    links = links.fillna('None')\n",
    "    links = links[~links.Description.str.contains('|'.join(discard))]\n",
    "    twitter_scrapes_preds = pd.merge(pred_df, links, on='URL')\n",
    "    twitter_scrapes_preds.to_pickle(f'{path}LOGREG_RELEVANCE/SCRAPES/{filename}.pkl')\n",
    "    print(len(twitter_scrapes_preds))\n",
    "    return twitter_scrapes_preds\n",
    "\n",
    "def twitter_predictions(path, filename, p_input, p_feature, score, discard, filter):\n",
    "    \"\"\" Predict relevant tweets using a pickled model based on Logistic regression and TF-IDF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_input: \n",
    "        dataframe of the prediction set\n",
    "    p_feature: \n",
    "        df column, text of tweet or description of the resource\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    score: int\n",
    "        which prediction score to filter the results by 1/0\n",
    "    discard: variable\n",
    "        a list of terms to check against to remove tweets\n",
    "    filter: str \n",
    "        a string against which to further filter predictions \n",
    "    \"\"\"\n",
    "    preds = lr_predict(path, filename, p_input, p_feature)\n",
    "    preds = preds.drop_duplicates(['tweet'], keep='last')\n",
    "    preds = preds.loc[preds['Prediction'] == score]\n",
    "    preds = preds[~preds.URL.str.contains('|'.join(discard))]\n",
    "    preds = preds.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "    preds = preds[['tweet', 'Prediction', 'Score', 'Probability', 'Input Length', 'URL', 'Search KW']]\n",
    "    if filter != '':\n",
    "        preds = preds[preds['tweet'].str.contains(filter)]\n",
    "        preds = preds.reset_index(drop=True)\n",
    "    twitter_link_list = [link for link in preds['URL']]\n",
    "    print('Total tweets classified:', len(preds))\n",
    "    return preds, twitter_link_list\n",
    "\n",
    "def resource_predictions(path, filename, p_input, p_feature, score, discard, savefile):\n",
    "    \"\"\" Predict relevant URL descriptions using a pickled model based on Logistic regression and TF-IDF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_input: \n",
    "        dataframe of the prediction set\n",
    "    p_feature: \n",
    "        df column, text of tweet or description of the resource\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    score: int\n",
    "        which prediction score to filter the results by 1/0\n",
    "    discard: variable\n",
    "        a list of terms to check against to remove tweets\n",
    "    savefile: str\n",
    "        name for the final csv to be saved under \n",
    "    \"\"\"\n",
    "    if len(filename) == 0:\n",
    "        return 'Sorry no URLs to classify!'\n",
    "    preds = lr_predict(path, filename, p_input, p_feature)\n",
    "    preds = preds.drop_duplicates(['Description'], keep='last')\n",
    "    preds = preds.loc[preds['Description'] != '']\n",
    "    preds = preds.loc[preds['Prediction'] == score]\n",
    "    preds = preds[~preds.URL.str.contains('|'.join(discard))]\n",
    "    preds = preds[~preds.Title.str.contains('|'.join(discard))]\n",
    "    preds = preds.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "    preds.to_csv(f'{path}LOGREG_RELEVANCE/PREDICTIONS/{savefile}.csv')\n",
    "    print(preds)\n",
    "    return preds\n",
    "\n",
    "def tweets_to_classify(path, filetype):   \n",
    "    \"\"\" Merge all tweet searches together.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: \n",
    "        for raw searches folder\n",
    "    filetype: \n",
    "        the ending of the files to load, you can call just .pkl or also the date tag from file names\n",
    "    \"\"\"  \n",
    "    raw_searches = path+'TWITTER_SEARCHES/RAW_SEARCHES/'\n",
    "    result = pd.DataFrame()\n",
    "    tweets_to_classify = pd.DataFrame()\n",
    "    for file in os.listdir(raw_searches):\n",
    "        if file.endswith(filetype):\n",
    "            result = pd.read_pickle(raw_searches+file)\n",
    "        tweets_to_classify = pd.concat([tweets_to_classify, result])\n",
    "        tweets_to_classify = tweets_to_classify.reset_index(drop=True)\n",
    "    print('Total tweets to classify:', len(tweets_to_classify))\n",
    "    return tweets_to_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training twitter and descriptions classifiers\n",
    "\n",
    "This is a ONE TIME operation. The models are pickled and loaded later to predict new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       538\n",
      "           1       0.94      0.95      0.95       786\n",
      "\n",
      "    accuracy                           0.94      1324\n",
      "   macro avg       0.93      0.93      0.93      1324\n",
      "weighted avg       0.93      0.94      0.93      1324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one time training on twitter\n",
    "#twitter_training_model = LogRegSearches.train(twitter_training_set_v1, 'tweet', 'Target', 10, 'precision', 250, None, 'twitter_v1_250maxfeats', path)\n",
    "\n",
    "# one time training on resources\n",
    "resource_training_model = LogRegSearches.train(archive_desc_training_v2, 'Description', 'Target', 10, 'precision', 1000, 'resources_v2_1Kmaxfeats', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       538\n",
      "           1       0.92      0.96      0.94       786\n",
      "\n",
      "    accuracy                           0.93      1324\n",
      "   macro avg       0.93      0.92      0.93      1324\n",
      "weighted avg       0.93      0.93      0.93      1324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=1000) \n",
    "x_train = tfidf_transformer.fit_transform(archive_desc_training_v2['Description'])\n",
    "y_train = archive_desc_training_v2['Target'].values\n",
    "model = make_pipeline(LogisticRegression(solver='liblinear', random_state=44))\n",
    "scores = cross_validate(model, x_train, y_train, scoring='precision', cv=10)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = cross_val_predict(model, x_train, y_train, cv=10)\n",
    "report = classification_report(y_train, y_pred)\n",
    "print('report:', report, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Twitter\n",
    "\n",
    "Calls Twitter API with the list of keywords and returns the table `prediction_twitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load token\n",
    "token = 'AAAAAAAAAAAAAAAAAAAAAJgsNAEAAAAAQcsgbUnOJJmqmU483%2F8x6n9V1i8%3Df0qaEo9cV1sWP4eyNQ6E9s8BiRjvFTSN9mSqithe8uIXSNP68x'\n",
    "#a selection of keywords based on MJI and musoW datasets\n",
    "\n",
    "#choose KW list \n",
    "##best performing list based on v1 tests\n",
    "better_keywords = ['audio file', 'music archive', 'music collection', 'music library', 'sheet music', 'sound archive', 'sound recording']\n",
    "\n",
    "#custom timeframe for searching\n",
    "start = ['2022-05-01T00:00:00.000Z', '2022-05-02T00:00:00.000Z', '2022-05-03T00:00:00.000Z', '2022-05-04T00:00:00.000Z', '2022-05-05T00:00:00.000Z', '2022-05-06T00:00:00.000Z', '2022-05-07T00:00:00.000Z']\n",
    "end = ['2022-05-01T23:59:59.000Z', '2022-05-02T23:59:59.000Z', '2022-05-03T23:59:59.000Z', '2022-05-04T23:59:59.000Z', '2022-05-05T23:59:59.000Z', '2022-05-06T23:59:59.000Z', '2022-05-07T23:59:59.000Z']\n",
    "\n",
    "#choose search option \n",
    "## search last week\n",
    "LogRegSearches.search_twitter_weekly(token, better_keywords, 500, 500)\n",
    "## search custom timeframe\n",
    "LogRegSearches.search_twitter_custom(token, better_keywords, start, end, 500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets to classify: 3679\n",
      "Total tweets to classify: 1310\n"
     ]
    }
   ],
   "source": [
    "#load all search results into a single dataframe \n",
    "classified_tweets = LogRegSearches.tweets_to_classify(f'{path}TWITTER_SEARCHES/RAW_SEARCHES_ARCHIVE/April 2022 searches v1 (500)/', '2022-04-01_4-30.pkl')\n",
    "classified_tweets_2 = LogRegSearches.tweets_to_classify(f'{path}TWITTER_SEARCHES/RAW_SEARCHES_ARCHIVE/Jan - March 2022 searches v1 (50)/', '2022-01-01_3-31.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all search results into a single dataframe \n",
    "tweets_to_classify = LogRegSearches.tweets_to_classify(path, '2022-05-01_5-05.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_tweets = pd.concat([classified_tweets, classified_tweets_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets classified: 1507\n"
     ]
    }
   ],
   "source": [
    "#run classification and get links from results\n",
    "predicted_tweets, twitter_link_list = LogRegSearches.predict_twitter(path, 'twitter_v1_100maxfeats', classified_tweets, 'tweet', 1, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "      <th>URL</th>\n",
       "      <th>Search KW</th>\n",
       "      <th>tweet date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BGM素材] Japanese-Style Music Collection　https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.506680</td>\n",
       "      <td>63</td>\n",
       "      <td>https://www.dlsite.com/home/dlaf/=/t/s/link/wo...</td>\n",
       "      <td>\"music collection\" -is:retweet</td>\n",
       "      <td>2022-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yo check out Ade's  music collection https://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023895</td>\n",
       "      <td>0.505973</td>\n",
       "      <td>60</td>\n",
       "      <td>https://music.sleevenote.com/@ade</td>\n",
       "      <td>\"music collection\" -is:retweet</td>\n",
       "      <td>2022-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relaxing With Chinese Bamboo Flute, Guzheng, E...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.505867</td>\n",
       "      <td>106</td>\n",
       "      <td>https://apps.gtarcade.com/v1/#/post/53386?anch...</td>\n",
       "      <td>\"music collection\" -is:retweet</td>\n",
       "      <td>2022-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Relaxing With Chinese Bamboo Flute, Guzheng, E...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.505867</td>\n",
       "      <td>106</td>\n",
       "      <td>https://apps.gtarcade.com/v1/#/post/53386?anchor=</td>\n",
       "      <td>\"music collection\" -is:retweet</td>\n",
       "      <td>2022-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IUMA (Internet Underground Music Archive) Coll...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>76</td>\n",
       "      <td>https://ift.tt/33IiRug</td>\n",
       "      <td>\"music archive\" -is:retweet</td>\n",
       "      <td>2022-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Everyone can move to the music!\n",
       "\n",
       "This poster h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.500012</td>\n",
       "      <td>218</td>\n",
       "      <td>https://www.twinkl.co.uk/l/gatw9</td>\n",
       "      <td>\"music information\" -is:retweet</td>\n",
       "      <td>2022-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Royalty Free Music library that will be with y...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.500012</td>\n",
       "      <td>241</td>\n",
       "      <td>https://music-for-video.com</td>\n",
       "      <td>\"music library\" -is:retweet</td>\n",
       "      <td>2022-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>did u read this. if not, its prob helpful htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.500010</td>\n",
       "      <td>66</td>\n",
       "      <td>https://www.vulture.com/article/clifford-marti...</td>\n",
       "      <td>\"oral history\" -is:retweet</td>\n",
       "      <td>2022-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Julian Urbano did some good work on this sam...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>171</td>\n",
       "      <td>https://www.slideshare.net/caerolus/statistica...</td>\n",
       "      <td>\"music information\" -is:retweet</td>\n",
       "      <td>2022-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Lawson Entertainment Inc. “Theatrical version ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.500002</td>\n",
       "      <td>251</td>\n",
       "      <td>https://re-how.net/all/1779774/</td>\n",
       "      <td>\"music information\" -is:retweet</td>\n",
       "      <td>2022-03-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  Prediction     Score  \\\n",
       "0    [BGM素材] Japanese-Style Music Collection　https:...           1  0.026723   \n",
       "1    Yo check out Ade's  music collection https://t...           1  0.023895   \n",
       "2    Relaxing With Chinese Bamboo Flute, Guzheng, E...           1  0.023468   \n",
       "3    Relaxing With Chinese Bamboo Flute, Guzheng, E...           1  0.023468   \n",
       "4    IUMA (Internet Underground Music Archive) Coll...           1  0.023168   \n",
       "..                                                 ...         ...       ...   \n",
       "430  Everyone can move to the music!\n",
       "\n",
       "This poster h...           1  0.000050   \n",
       "431  Royalty Free Music library that will be with y...           1  0.000046   \n",
       "432   did u read this. if not, its prob helpful htt...           1  0.000040   \n",
       "433    Julian Urbano did some good work on this sam...           1  0.000013   \n",
       "434  Lawson Entertainment Inc. “Theatrical version ...           1  0.000010   \n",
       "\n",
       "     Probability  Input Length  \\\n",
       "0       0.506680            63   \n",
       "1       0.505973            60   \n",
       "2       0.505867           106   \n",
       "3       0.505867           106   \n",
       "4       0.505792            76   \n",
       "..           ...           ...   \n",
       "430     0.500012           218   \n",
       "431     0.500012           241   \n",
       "432     0.500010            66   \n",
       "433     0.500003           171   \n",
       "434     0.500002           251   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://www.dlsite.com/home/dlaf/=/t/s/link/wo...   \n",
       "1                    https://music.sleevenote.com/@ade   \n",
       "2    https://apps.gtarcade.com/v1/#/post/53386?anch...   \n",
       "3    https://apps.gtarcade.com/v1/#/post/53386?anchor=   \n",
       "4                               https://ift.tt/33IiRug   \n",
       "..                                                 ...   \n",
       "430                   https://www.twinkl.co.uk/l/gatw9   \n",
       "431                        https://music-for-video.com   \n",
       "432  https://www.vulture.com/article/clifford-marti...   \n",
       "433  https://www.slideshare.net/caerolus/statistica...   \n",
       "434                    https://re-how.net/all/1779774/   \n",
       "\n",
       "                           Search KW  tweet date  \n",
       "0     \"music collection\" -is:retweet  2022-02-27  \n",
       "1     \"music collection\" -is:retweet  2022-03-30  \n",
       "2     \"music collection\" -is:retweet  2022-03-30  \n",
       "3     \"music collection\" -is:retweet  2022-03-30  \n",
       "4        \"music archive\" -is:retweet  2022-01-24  \n",
       "..                               ...         ...  \n",
       "430  \"music information\" -is:retweet  2022-03-15  \n",
       "431      \"music library\" -is:retweet  2022-03-30  \n",
       "432       \"oral history\" -is:retweet  2022-02-27  \n",
       "433  \"music information\" -is:retweet  2022-02-10  \n",
       "434  \"music information\" -is:retweet  2022-03-23  \n",
       "\n",
       "[435 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URL list and return a DF for resource classification\n",
    "scraped_links = LogRegSearches.scrape_links(twitter_link_list, predicted_tweets, 'all_keywords_2022-01-01_04-31_scrapes_maxfeats200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scraped_links) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "raw_searches = path+'LOGREG_RELEVANCE/SCRAPES/Jan-April 2022 baseline/'\n",
    "result = pd.DataFrame()\n",
    "scraped_links = pd.DataFrame()\n",
    "filetype = 'scrapes.pkl'\n",
    "for file in os.listdir(raw_searches):\n",
    "    if file.endswith(filetype):\n",
    "        result = pd.read_pickle(raw_searches+file)\n",
    "        scraped_links = pd.concat([scraped_links, result])\n",
    "        scraped_links = scraped_links.reset_index(drop=True)\n",
    "len(scraped_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_resources = LogRegSearches.predict_resource(path, 'resources_v2_100maxfeats', scraped_links, 'Description', 1, 'all_keywords_2022-01-01_04-30_desc_v2_100maxfeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "      <th>URL</th>\n",
       "      <th>Search KW</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://t.co/OkEHI6XQu3 Music archive Wolfgang...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.489973</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>189</td>\n",
       "      <td>http://www.opusip.co.uk/2022/01/25/music-archi...</td>\n",
       "      <td>\"music archive\" -is:retweet</td>\n",
       "      <td>Music archive Wolfgang's Vault resolves copyri...</td>\n",
       "      <td>25th January 2022\\nMusic archive Wolfgang’s Va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online concert archive Wolfgang's Vault has re...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.473366</td>\n",
       "      <td>0.998458</td>\n",
       "      <td>308</td>\n",
       "      <td>https://reut.rs/3rTJWTx</td>\n",
       "      <td>\"music archive\" -is:retweet</td>\n",
       "      <td>Music archive Wolfgang's Vault resolves copyri...</td>\n",
       "      <td>Rocker Greg Kihn sued online archive over all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video Interview:\n",
       "Florida Sound Archive With Sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.960977</td>\n",
       "      <td>0.997429</td>\n",
       "      <td>113</td>\n",
       "      <td>https://www.musiceternal.com/News/2022/Florida...</td>\n",
       "      <td>\"sound archive\" -is:retweet</td>\n",
       "      <td>Florida Sound Archive With Sam Rosenthal</td>\n",
       "      <td>Interview with Sam Rosenthal of Projekt Record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inmates at HMP Perth engage with archive birds...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.949058</td>\n",
       "      <td>0.997398</td>\n",
       "      <td>216</td>\n",
       "      <td>https://www.listentosteve.com/doingbird</td>\n",
       "      <td>\"oral history\" -is:retweet</td>\n",
       "      <td>Steve Urquhart // doing bird</td>\n",
       "      <td>Inmates at HMP Perth engage with archive bird...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With first-hand access to the original film ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.705338</td>\n",
       "      <td>0.996683</td>\n",
       "      <td>285</td>\n",
       "      <td>https://buff.ly/38mYNQm</td>\n",
       "      <td>\"archive collection\" -is:retweet</td>\n",
       "      <td>Introducing… THE ARCHIVE COLLECTION – Regal Robot</td>\n",
       "      <td>The Archive Collection – by Regal Robot™Intro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Hard to believe there isn't, although it appe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021968</td>\n",
       "      <td>0.505492</td>\n",
       "      <td>298</td>\n",
       "      <td>https://digital.library.unt.edu/ark:/67531/met...</td>\n",
       "      <td>\"digital library\" -is:retweet</td>\n",
       "      <td>Norma and Mel Gabler: The Development and Caus...</td>\n",
       "      <td>The problem of this study was to trace throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>Join Delyna Baxter as she discusses the true h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.502454</td>\n",
       "      <td>193</td>\n",
       "      <td>http://bit.ly/register-delyna22</td>\n",
       "      <td>\"oral history\" -is:retweet</td>\n",
       "      <td>Webinar Registration - Zoom</td>\n",
       "      <td>Zoom is a full-featured, easy-to-use, engagin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>NowPlaying blues : Realize (Digital Edition Bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>238</td>\n",
       "      <td>https://cowboysjukejoint.com/</td>\n",
       "      <td>\"digital edition\" -is:retweet</td>\n",
       "      <td>Cowboy's Juke Joint - Playing The Harder side ...</td>\n",
       "      <td>Cowboy’s Juke Joint is an independent non-com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>Concerts of the ensemble Bis-Quit band from St...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.500172</td>\n",
       "      <td>239</td>\n",
       "      <td>https://en.rwp.agency/news/953/</td>\n",
       "      <td>\"music culture\" -is:retweet</td>\n",
       "      <td>Concerts of the ensemble Bis-Quit band from St...</td>\n",
       "      <td>Kyrgyzstan will hold concerts of the Bis-Quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Rationality [sound Recording] : What It Is, Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>254</td>\n",
       "      <td>https://cincinnatilibrary.bibliocommons.com/v2...</td>\n",
       "      <td>\"sound recording\" -is:retweet</td>\n",
       "      <td>Rationality | Cincinnati &amp; Hamilton County Pub...</td>\n",
       "      <td>Today humanity is reaching new heights of sci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  Prediction     Score  \\\n",
       "0    https://t.co/OkEHI6XQu3 Music archive Wolfgang...           1  7.489973   \n",
       "1    Online concert archive Wolfgang's Vault has re...           1  6.473366   \n",
       "2    Video Interview:\n",
       "Florida Sound Archive With Sa...           1  5.960977   \n",
       "3    Inmates at HMP Perth engage with archive birds...           1  5.949058   \n",
       "4    With first-hand access to the original film ar...           1  5.705338   \n",
       "..                                                 ...         ...       ...   \n",
       "480   Hard to believe there isn't, although it appe...           1  0.021968   \n",
       "481  Join Delyna Baxter as she discusses the true h...           1  0.009815   \n",
       "482  NowPlaying blues : Realize (Digital Edition Bo...           1  0.008761   \n",
       "483  Concerts of the ensemble Bis-Quit band from St...           1  0.000688   \n",
       "484  Rationality [sound Recording] : What It Is, Wh...           1  0.000175   \n",
       "\n",
       "     Probability  Input Length  \\\n",
       "0       0.999442           189   \n",
       "1       0.998458           308   \n",
       "2       0.997429           113   \n",
       "3       0.997398           216   \n",
       "4       0.996683           285   \n",
       "..           ...           ...   \n",
       "480     0.505492           298   \n",
       "481     0.502454           193   \n",
       "482     0.502190           238   \n",
       "483     0.500172           239   \n",
       "484     0.500044           254   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    http://www.opusip.co.uk/2022/01/25/music-archi...   \n",
       "1                              https://reut.rs/3rTJWTx   \n",
       "2    https://www.musiceternal.com/News/2022/Florida...   \n",
       "3              https://www.listentosteve.com/doingbird   \n",
       "4                              https://buff.ly/38mYNQm   \n",
       "..                                                 ...   \n",
       "480  https://digital.library.unt.edu/ark:/67531/met...   \n",
       "481                    http://bit.ly/register-delyna22   \n",
       "482                      https://cowboysjukejoint.com/   \n",
       "483                    https://en.rwp.agency/news/953/   \n",
       "484  https://cincinnatilibrary.bibliocommons.com/v2...   \n",
       "\n",
       "                            Search KW  \\\n",
       "0         \"music archive\" -is:retweet   \n",
       "1         \"music archive\" -is:retweet   \n",
       "2         \"sound archive\" -is:retweet   \n",
       "3          \"oral history\" -is:retweet   \n",
       "4    \"archive collection\" -is:retweet   \n",
       "..                                ...   \n",
       "480     \"digital library\" -is:retweet   \n",
       "481        \"oral history\" -is:retweet   \n",
       "482     \"digital edition\" -is:retweet   \n",
       "483       \"music culture\" -is:retweet   \n",
       "484     \"sound recording\" -is:retweet   \n",
       "\n",
       "                                                 Title  \\\n",
       "0    Music archive Wolfgang's Vault resolves copyri...   \n",
       "1    Music archive Wolfgang's Vault resolves copyri...   \n",
       "2             Florida Sound Archive With Sam Rosenthal   \n",
       "3                         Steve Urquhart // doing bird   \n",
       "4    Introducing… THE ARCHIVE COLLECTION – Regal Robot   \n",
       "..                                                 ...   \n",
       "480  Norma and Mel Gabler: The Development and Caus...   \n",
       "481                        Webinar Registration - Zoom   \n",
       "482  Cowboy's Juke Joint - Playing The Harder side ...   \n",
       "483  Concerts of the ensemble Bis-Quit band from St...   \n",
       "484  Rationality | Cincinnati & Hamilton County Pub...   \n",
       "\n",
       "                                           Description  \n",
       "0    25th January 2022\\nMusic archive Wolfgang’s Va...  \n",
       "1     Rocker Greg Kihn sued online archive over all...  \n",
       "2    Interview with Sam Rosenthal of Projekt Record...  \n",
       "3     Inmates at HMP Perth engage with archive bird...  \n",
       "4     The Archive Collection – by Regal Robot™Intro...  \n",
       "..                                                 ...  \n",
       "480   The problem of this study was to trace throug...  \n",
       "481   Zoom is a full-featured, easy-to-use, engagin...  \n",
       "482   Cowboy’s Juke Joint is an independent non-com...  \n",
       "483   Kyrgyzstan will hold concerts of the Bis-Quit...  \n",
       "484   Today humanity is reaching new heights of sci...  \n",
       "\n",
       "[485 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare final results for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = pd.read_csv(path+'LOGREG_RELEVANCE/FINAL_RESULTS/Jan-April_2022_assessed_by_Marilena.csv')\n",
    "baseline['source'] = 'baseline'\n",
    "baseline_true = baseline.loc[baseline['Add to musoW'] != 'no']\n",
    "baseline_false = baseline.loc[baseline['Add to musoW'] == 'no']\n",
    "\n",
    "evaluate = pd.read_csv(path+'LOGREG_RELEVANCE/PREDICTIONS/all_keywords_2022-01-01_04-30_desc_v2_100maxfeats.csv')\n",
    "evaluate['source'] = 'new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_true = baseline_true['URL'].to_dict()\n",
    "baseline_true = dict([(value, key) for key, value in baseline_true.items()])\n",
    "baseline_true.update((k,'true positive') for k in baseline_true)\n",
    "baseline_false = baseline_false['URL'].to_dict()\n",
    "baseline_false = dict([(value, key) for key, value in baseline_false.items()])\n",
    "baseline_false.update((k,'false positive') for k in baseline_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate['sort'] = evaluate['URL'].map(baseline_true)\n",
    "evaluate.loc[evaluate['sort'] != 'true positive', 'sort'] = evaluate['URL'].map(baseline_false)\n",
    "evaluate['sort'] = evaluate['sort'].fillna('new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "      <th>URL</th>\n",
       "      <th>Search KW</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>source</th>\n",
       "      <th>sort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/OkEHI6XQu3 Music archive Wolfgang...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.489973</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>189</td>\n",
       "      <td>http://www.opusip.co.uk/2022/01/25/music-archi...</td>\n",
       "      <td>\"music archive\" -is:retweet</td>\n",
       "      <td>Music archive Wolfgang's Vault resolves copyri...</td>\n",
       "      <td>25th January 2022\\nMusic archive Wolfgang’s Va...</td>\n",
       "      <td>new</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Online concert archive Wolfgang's Vault has re...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.473366</td>\n",
       "      <td>0.998458</td>\n",
       "      <td>308</td>\n",
       "      <td>https://reut.rs/3rTJWTx</td>\n",
       "      <td>\"music archive\" -is:retweet</td>\n",
       "      <td>Music archive Wolfgang's Vault resolves copyri...</td>\n",
       "      <td>Rocker Greg Kihn sued online archive over all...</td>\n",
       "      <td>new</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Video Interview:\\nFlorida Sound Archive With S...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.960977</td>\n",
       "      <td>0.997429</td>\n",
       "      <td>113</td>\n",
       "      <td>https://www.musiceternal.com/News/2022/Florida...</td>\n",
       "      <td>\"sound archive\" -is:retweet</td>\n",
       "      <td>Florida Sound Archive With Sam Rosenthal</td>\n",
       "      <td>Interview with Sam Rosenthal of Projekt Record...</td>\n",
       "      <td>new</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inmates at HMP Perth engage with archive birds...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.949058</td>\n",
       "      <td>0.997398</td>\n",
       "      <td>216</td>\n",
       "      <td>https://www.listentosteve.com/doingbird</td>\n",
       "      <td>\"oral history\" -is:retweet</td>\n",
       "      <td>Steve Urquhart // doing bird</td>\n",
       "      <td>Inmates at HMP Perth engage with archive bird...</td>\n",
       "      <td>new</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>With first-hand access to the original film ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.705338</td>\n",
       "      <td>0.996683</td>\n",
       "      <td>285</td>\n",
       "      <td>https://buff.ly/38mYNQm</td>\n",
       "      <td>\"archive collection\" -is:retweet</td>\n",
       "      <td>Introducing… THE ARCHIVE COLLECTION – Regal Robot</td>\n",
       "      <td>The Archive Collection – by Regal Robot™Intro...</td>\n",
       "      <td>new</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>480</td>\n",
       "      <td>Hard to believe there isn't, although it appe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021968</td>\n",
       "      <td>0.505492</td>\n",
       "      <td>298</td>\n",
       "      <td>https://digital.library.unt.edu/ark:/67531/met...</td>\n",
       "      <td>\"digital library\" -is:retweet</td>\n",
       "      <td>Norma and Mel Gabler: The Development and Caus...</td>\n",
       "      <td>The problem of this study was to trace throug...</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>481</td>\n",
       "      <td>Join Delyna Baxter as she discusses the true h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.502454</td>\n",
       "      <td>193</td>\n",
       "      <td>http://bit.ly/register-delyna22</td>\n",
       "      <td>\"oral history\" -is:retweet</td>\n",
       "      <td>Webinar Registration - Zoom</td>\n",
       "      <td>Zoom is a full-featured, easy-to-use, engagin...</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>NowPlaying blues : Realize (Digital Edition Bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>238</td>\n",
       "      <td>https://cowboysjukejoint.com/</td>\n",
       "      <td>\"digital edition\" -is:retweet</td>\n",
       "      <td>Cowboy's Juke Joint - Playing The Harder side ...</td>\n",
       "      <td>Cowboy’s Juke Joint is an independent non-com...</td>\n",
       "      <td>new</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>483</td>\n",
       "      <td>Concerts of the ensemble Bis-Quit band from St...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.500172</td>\n",
       "      <td>239</td>\n",
       "      <td>https://en.rwp.agency/news/953/</td>\n",
       "      <td>\"music culture\" -is:retweet</td>\n",
       "      <td>Concerts of the ensemble Bis-Quit band from St...</td>\n",
       "      <td>Kyrgyzstan will hold concerts of the Bis-Quit...</td>\n",
       "      <td>new</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>484</td>\n",
       "      <td>Rationality [sound Recording] : What It Is, Wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>254</td>\n",
       "      <td>https://cincinnatilibrary.bibliocommons.com/v2...</td>\n",
       "      <td>\"sound recording\" -is:retweet</td>\n",
       "      <td>Rationality | Cincinnati &amp; Hamilton County Pub...</td>\n",
       "      <td>Today humanity is reaching new heights of sci...</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                              tweet  \\\n",
       "0             0  https://t.co/OkEHI6XQu3 Music archive Wolfgang...   \n",
       "1             1  Online concert archive Wolfgang's Vault has re...   \n",
       "2             2  Video Interview:\\nFlorida Sound Archive With S...   \n",
       "3             3  Inmates at HMP Perth engage with archive birds...   \n",
       "4             4  With first-hand access to the original film ar...   \n",
       "..          ...                                                ...   \n",
       "480         480   Hard to believe there isn't, although it appe...   \n",
       "481         481  Join Delyna Baxter as she discusses the true h...   \n",
       "482         482  NowPlaying blues : Realize (Digital Edition Bo...   \n",
       "483         483  Concerts of the ensemble Bis-Quit band from St...   \n",
       "484         484  Rationality [sound Recording] : What It Is, Wh...   \n",
       "\n",
       "     Prediction     Score  Probability  Input Length  \\\n",
       "0             1  7.489973     0.999442           189   \n",
       "1             1  6.473366     0.998458           308   \n",
       "2             1  5.960977     0.997429           113   \n",
       "3             1  5.949058     0.997398           216   \n",
       "4             1  5.705338     0.996683           285   \n",
       "..          ...       ...          ...           ...   \n",
       "480           1  0.021968     0.505492           298   \n",
       "481           1  0.009815     0.502454           193   \n",
       "482           1  0.008761     0.502190           238   \n",
       "483           1  0.000688     0.500172           239   \n",
       "484           1  0.000175     0.500044           254   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    http://www.opusip.co.uk/2022/01/25/music-archi...   \n",
       "1                              https://reut.rs/3rTJWTx   \n",
       "2    https://www.musiceternal.com/News/2022/Florida...   \n",
       "3              https://www.listentosteve.com/doingbird   \n",
       "4                              https://buff.ly/38mYNQm   \n",
       "..                                                 ...   \n",
       "480  https://digital.library.unt.edu/ark:/67531/met...   \n",
       "481                    http://bit.ly/register-delyna22   \n",
       "482                      https://cowboysjukejoint.com/   \n",
       "483                    https://en.rwp.agency/news/953/   \n",
       "484  https://cincinnatilibrary.bibliocommons.com/v2...   \n",
       "\n",
       "                            Search KW  \\\n",
       "0         \"music archive\" -is:retweet   \n",
       "1         \"music archive\" -is:retweet   \n",
       "2         \"sound archive\" -is:retweet   \n",
       "3          \"oral history\" -is:retweet   \n",
       "4    \"archive collection\" -is:retweet   \n",
       "..                                ...   \n",
       "480     \"digital library\" -is:retweet   \n",
       "481        \"oral history\" -is:retweet   \n",
       "482     \"digital edition\" -is:retweet   \n",
       "483       \"music culture\" -is:retweet   \n",
       "484     \"sound recording\" -is:retweet   \n",
       "\n",
       "                                                 Title  \\\n",
       "0    Music archive Wolfgang's Vault resolves copyri...   \n",
       "1    Music archive Wolfgang's Vault resolves copyri...   \n",
       "2             Florida Sound Archive With Sam Rosenthal   \n",
       "3                         Steve Urquhart // doing bird   \n",
       "4    Introducing… THE ARCHIVE COLLECTION – Regal Robot   \n",
       "..                                                 ...   \n",
       "480  Norma and Mel Gabler: The Development and Caus...   \n",
       "481                        Webinar Registration - Zoom   \n",
       "482  Cowboy's Juke Joint - Playing The Harder side ...   \n",
       "483  Concerts of the ensemble Bis-Quit band from St...   \n",
       "484  Rationality | Cincinnati & Hamilton County Pub...   \n",
       "\n",
       "                                           Description source            sort  \n",
       "0    25th January 2022\\nMusic archive Wolfgang’s Va...    new  false positive  \n",
       "1     Rocker Greg Kihn sued online archive over all...    new  false positive  \n",
       "2    Interview with Sam Rosenthal of Projekt Record...    new   true positive  \n",
       "3     Inmates at HMP Perth engage with archive bird...    new   true positive  \n",
       "4     The Archive Collection – by Regal Robot™Intro...    new  false positive  \n",
       "..                                                 ...    ...             ...  \n",
       "480   The problem of this study was to trace throug...    new             new  \n",
       "481   Zoom is a full-featured, easy-to-use, engagin...    new             new  \n",
       "482   Cowboy’s Juke Joint is an independent non-com...    new  false positive  \n",
       "483   Kyrgyzstan will hold concerts of the Bis-Quit...    new  false positive  \n",
       "484   Today humanity is reaching new heights of sci...    new             new  \n",
       "\n",
       "[485 rows x 12 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.to_csv(path+'LOGREG_RELEVANCE/FINAL_RESULTS/Jan-April_2022_desc_maxfeats_100.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
