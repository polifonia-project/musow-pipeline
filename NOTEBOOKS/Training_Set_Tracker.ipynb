{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING SET TRACKING\n",
    "\n",
    "- This notebook tracks the evolution of the training sets for the musoW discovery pipeline for easy reference and (re)construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "import pandas as pd\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description training set v1 \n",
    "- musoW descriptions as positives\n",
    "- MJI descriptions as negatives \n",
    "- base version + extended version (additions to negatives) + even version (for balancing)\n",
    "\n",
    "DEPRECATED - DO NOT USE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mji csv and grab needed columns\n",
    "df_mji = pd.read_csv(path+'MJI/MJI_data.csv', keep_default_na=False, dtype='string')\n",
    "df_mji_small = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_mji_small['Title'] = df_mji['Title'].str.lower().str.strip()\n",
    "df_mji_small['Description'] = df_mji['Description'].str.lower().str.strip()\n",
    "df_mji_small['URL'] = df_mji['URL'].str.lower().str.strip()\n",
    "\n",
    "#read musow json dump and grab needed columns\n",
    "with open(path+'MUSOW/musow_name_desc_url_cat.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "musow_names = [result['name']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_desc = [result['description']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_url = [result['url']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "df_musow = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_musow['Title'] = musow_names\n",
    "df_musow['Description'] = musow_desc\n",
    "df_musow['URL'] = musow_url\n",
    "df_musow = df_musow.astype('string')\n",
    "\n",
    "#remove musow duplicates from MJI set \n",
    "mji_training_set = df_mji_small[~df_mji_small['Title'].isin(df_musow['Title'])].dropna()\n",
    "\n",
    "#create positive and negative base sets, add target column \n",
    "positive_df = df_musow.copy()\n",
    "positive_df['Target'] = '1'\n",
    "negative_df = mji_training_set.copy()\n",
    "negative_df['Target'] = '0'\n",
    "\n",
    "#create positive and negative sets w/ additions, add target column \n",
    "ismir_df = pd.read_pickle(path+'GH_PICKLES/ismir.pkl')\n",
    "ismir_df = ismir_df[~ismir_df['Title'].isin(df_musow['Title'])].dropna() \n",
    "positive_df_adds = pd.concat([df_musow, ismir_df]).reset_index(drop=True)\n",
    "positive_df_adds = positive_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "positive_df_adds['Target'] = '1'\n",
    "mji_additions_1 = pd.read_csv(path+'MJI/MJI_additions_for_LR.csv')\n",
    "mji_additions_1['Title'] = mji_additions_1['Title'].str.lower().str.strip()\n",
    "mji_additions_1['Description'] = mji_additions_1['Description'].str.lower().str.strip()\n",
    "mji_additions_1['URL'] = mji_additions_1['URL'].str.lower().str.strip()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(df_musow['Title'])].dropna()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(mji_training_set['Title'])].dropna()\n",
    "negative_df_adds = pd.concat([mji_training_set, mji_additions_1]).reset_index(drop=True)\n",
    "negative_df_adds = negative_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "negative_df_adds['Target'] = '0'\n",
    "\n",
    "#create the base and extended training sets, pickle for reuse\n",
    "training_set = pd.concat([positive_df, negative_df])\n",
    "training_set['Target'] = training_set['Target'].astype('int')\n",
    "training_set = training_set.reset_index(drop=True)\n",
    "training_set_adds = pd.concat([positive_df_adds, negative_df_adds])\n",
    "training_set_adds['Target'] = training_set_adds['Target'].astype('int')\n",
    "training_set_adds = training_set_adds.reset_index(drop=True)\n",
    "training_set.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/trainingset.pkl')\n",
    "training_set_adds.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/trainingset_extended.pkl')\n",
    "\n",
    "#create the even set, one base and one extended, pickle for reuse \n",
    "positive_df_2 = positive_df.sample(n=128, random_state=1)\n",
    "training_set_even = pd.concat([positive_df_2, negative_df])\n",
    "training_set_even['Target'] = training_set_even['Target'].astype('int')\n",
    "training_set_even = training_set_even.reset_index(drop=True)\n",
    "training_set_even.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/trainingset_even.pkl')\n",
    "\n",
    "positive_df_3 = positive_df.sample(n=272, random_state=1)\n",
    "training_set_even_adds = pd.concat([positive_df_3, negative_df_adds])\n",
    "training_set_even_adds['Target'] = training_set_even_adds['Target'].astype('int')\n",
    "training_set_even_adds = training_set_even_adds.reset_index(drop=True)\n",
    "training_set_even_adds.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/trainingset_even_extended.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description training set v2 + 3\n",
    "- musoW and MJI descriptions as positives, using even extended set from v1\n",
    "- manual and automatic scrapes from twitter searches for digital humanities, music companies, music industry as negatives for v2\n",
    "- summarized automatic scrapes of negatives for v3\n",
    "\n",
    "v3 is the version on which tests for V1 of the pipeline on Twitter were conducted, results of which are included in Marilena's report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW TRAINING SET V2\n",
    "new_neg_set = pd.read_excel(path+'LOGREG_RELEVANCE/TRAINING_SETS/non_archive_negative_set_v1.xlsx')\n",
    "new_neg_set = new_neg_set.drop_duplicates(subset=['Title'])\n",
    "new_neg_set = new_neg_set.drop_duplicates(subset=['Description'])\n",
    "new_neg_set['Target'] = '0'\n",
    "positive_set = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/trainingset_even_extended.pkl')\n",
    "positive_set['Target'] = '1'\n",
    "archive_desc_training_v1 = pd.concat([positive_set, new_neg_set])\n",
    "archive_desc_training_v1['Target'] = archive_desc_training_v1['Target'].astype('int')\n",
    "archive_desc_training_v1 = archive_desc_training_v1.reset_index(drop=True)\n",
    "archive_desc_training_v1.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/archive_desc_training_v1.pkl')\n",
    "\n",
    "#NEW TRAINING SET V3\n",
    "new_neg_set_2 = pd.read_excel(path+'LOGREG_RELEVANCE/TRAINING_SETS/non_archive_negative_set_v2.xlsx')\n",
    "new_neg_set_2 = new_neg_set_2.drop_duplicates(subset=['Title'])\n",
    "new_neg_set_2 = new_neg_set_2.drop_duplicates(subset=['Description'])\n",
    "new_neg_set_2['Target'] = '0'\n",
    "positive_set_2 = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/trainingset_extended.pkl')\n",
    "positive_set_2['Target'] = '1'\n",
    "archive_desc_training_v2 = pd.concat([positive_set_2, new_neg_set_2])\n",
    "archive_desc_training_v2['Target'] = archive_desc_training_v2['Target'].astype('int')\n",
    "archive_desc_training_v2 = archive_desc_training_v2.reset_index(drop=True)\n",
    "archive_desc_training_v2.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/archive_desc_training_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Training Set v1 \n",
    "- tweets from bigrams searches for positives (see keywords notebook for details of research)\n",
    "- tweets from searches for digital humanities, music companies, music industry for negatives (tied to v2+3 of description training set)\n",
    "\n",
    "v1 is the version on which tests for V1 of the pipeline on Twitter were conducted, results of which are included in Marilena's report.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative twitter training set\n",
    "dh = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/digital_humanities_2021.pkl')\n",
    "music_company = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/music_company_2021.pkl')\n",
    "twitter_neg = pd.concat([dh, music_company])\n",
    "twitter_neg = twitter_neg.loc[twitter_neg['lang'] == 'en']\n",
    "twitter_neg['Target'] = '0'\n",
    "twitter_neg = twitter_neg.sample(n=4379, random_state=56)\n",
    "twitter_neg = twitter_neg[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "#positive twitter training set \n",
    "music_collection = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_collection.pkl')\n",
    "song_dataset = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_song_dataset.pkl')\n",
    "sound_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS/twitter_sound_archive.pkl')\n",
    "digital_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_digital_archive.pkl')\n",
    "music_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_archive.pkl')\n",
    "digi_music_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_digital_music_archive.pkl')\n",
    "midi_file = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_midi_file.pkl')\n",
    "music_data = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_data.pkl')\n",
    "music_research = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS/twitter_music_research.pkl')\n",
    "music_dataset = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_dataset.pkl')\n",
    "twitter_pos = pd.concat([sound_archive, music_collection, digital_archive, music_archive, song_dataset, digi_music_archive, midi_file, music_data, music_research, music_dataset])\n",
    "twitter_pos = twitter_pos.loc[twitter_pos['lang'] == 'en']\n",
    "twitter_pos['Target'] = '1'\n",
    "twitter_pos = twitter_pos[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "#final twitter training set\n",
    "twitter_set = pd.concat([twitter_pos, twitter_neg])\n",
    "twitter_set['Target'] = twitter_set['Target'].astype('int')\n",
    "twitter_set = twitter_set.reset_index(drop=True)\n",
    "twitter_set.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/twitter_training_v1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Training Set v4 \n",
    "\n",
    "- Manual work:\n",
    "    - Lenghthen musow descs where possible \n",
    "    - Add some new MJI descs\n",
    "    - Add pos/neg from baseline test \n",
    "- Lower case  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW DESC TRAINING SET V4\n",
    "desc_neg_v4 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Description V4/Training Set Desc v4 - Negatives.csv')\n",
    "desc_neg_v4 = desc_neg_v4.drop(labels=['Notes', 'Source'], axis=1)\n",
    "desc_neg_v4['Title'] = desc_neg_v4['Title'].str.lower()\n",
    "desc_neg_v4['Description'] = desc_neg_v4['Description'].str.lower()\n",
    "desc_neg_v4 = desc_neg_v4.drop_duplicates(subset=['URL'])\n",
    "desc_neg_v4 = desc_neg_v4.drop_duplicates(subset=['Title'])\n",
    "desc_neg_v4 = desc_neg_v4.drop_duplicates(subset=['Description'])\n",
    "\n",
    "desc_pos_v4 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Description V4/Training Set Desc v4 - Positives.csv')\n",
    "desc_pos_v4 = desc_pos_v4.drop(labels=['Notes', 'Source'], axis=1)\n",
    "desc_pos_v4['Title'] = desc_pos_v4['Title'].str.lower()\n",
    "desc_pos_v4['Description'] = desc_pos_v4['Description'].str.lower()\n",
    "desc_pos_v4 = desc_pos_v4.drop_duplicates(subset=['URL'])\n",
    "desc_pos_v4 = desc_pos_v4.drop_duplicates(subset=['Title'])\n",
    "desc_pos_v4 = desc_pos_v4.drop_duplicates(subset=['Description'])\n",
    "\n",
    "archive_desc_training_v4 = pd.concat([desc_neg_v4, desc_pos_v4])\n",
    "archive_desc_training_v4['Target'] = archive_desc_training_v4['Target'].astype('int')\n",
    "archive_desc_training_v4 = archive_desc_training_v4.reset_index(drop=True)\n",
    "\n",
    "archive_desc_training_v4.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/archive_desc_training_v4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Training Set v2\n",
    "\n",
    "- Manual work:\n",
    "    - Clean up positives from v1 set, remove all dupes, spam, non english \n",
    "    - Take negs from baseline test + max feats 100 test, clean up \n",
    "- Lower case\n",
    "- Remove emojis   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW DESC TRAINING SET V4\n",
    "twitter_neg_v2 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Twitter V2/Twitter v2 - Negatives.csv')\n",
    "twitter_neg_v2 = twitter_neg_v2.drop(labels=['Source'], axis=1)\n",
    "twitter_neg_v2['tweet'] = twitter_neg_v2['tweet'].str.lower()\n",
    "twitter_neg_v2['tweet'] = twitter_neg_v2['tweet'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "twitter_neg_v2 = twitter_neg_v2.drop_duplicates(subset=['tweet'])\n",
    "twitter_neg_v2['Target'] = twitter_neg_v2['Target'].astype(int)\n",
    "\n",
    "twitter_pos_v2 = pd.read_csv(path+'LOGREG_RELEVANCE/TRAINING_SETS/Pipeline_v3_sets/Twitter V2/Twitter v2 - Positives.csv')\n",
    "twitter_pos_v2 = twitter_pos_v2.drop(labels=['Source'], axis=1)\n",
    "twitter_pos_v2['tweet'] = twitter_pos_v2['tweet'].str.lower()\n",
    "twitter_pos_v2['tweet'] = twitter_pos_v2['tweet'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "twitter_pos_v2 = twitter_pos_v2.drop_duplicates(subset=['tweet'])\n",
    "twitter_pos_v2['Target'] = twitter_pos_v2['Target'].astype(int)\n",
    "\n",
    "twitter_training_v2 = pd.concat([twitter_neg_v2, twitter_pos_v2])\n",
    "twitter_training_v2 = twitter_training_v2.reset_index(drop=True)\n",
    "\n",
    "#twitter_training_v2.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/twitter_training_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALT NEGATIVE \n",
    "\n",
    "twitter_neg_v2_alt = pd.read_pickle(path+'OLD_DO_NOT_USE/NEGATIVE/digital_humanities_2021.pkl')\n",
    "twitter_neg_v2_alt = twitter_neg_v2_alt.loc[twitter_neg_v2_alt['lang'] == 'en']\n",
    "twitter_neg_v2_alt['tweet'] = twitter_neg_v2_alt['tweet'].str.lower()\n",
    "twitter_neg_v2_alt['tweet'] = twitter_neg_v2_alt['tweet'].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "twitter_neg_v2_alt['Target'] = '0'\n",
    "twitter_neg_v2_alt['Target'] = twitter_neg_v2_alt['Target'].astype(int)\n",
    "twitter_neg_v2_alt = twitter_neg_v2_alt.sample(n=510, random_state=33)\n",
    "twitter_neg_v2_alt = twitter_neg_v2_alt[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "twitter_training_v2_alt = pd.concat([twitter_neg_v2_alt, twitter_pos_v2])\n",
    "twitter_training_v2_alt = twitter_training_v2_alt.reset_index(drop=True)\n",
    "\n",
    "twitter_training_v2_alt.to_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/twitter_training_v2_alt.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
