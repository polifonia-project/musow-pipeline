{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "- This notebook includes code to run searches against xml documents \n",
    "- Primarily tested for results from arxiv.org and Google Alerts RSS \n",
    "\n",
    "TO DO:\n",
    "- create functions similar to github/twitter to return usable DFs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML files (e.g. Arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import urllib, urllib.request\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open url and return it as a local xml file \n",
    "url = 'https://export.arxiv.org/api/query?search_query=%22music+collection%22&searchtype=abstract&abstracts=show&order=-announced_date_firs'\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n",
    "file = urllib.request.urlretrieve(url, 'arxiv.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse xml file and extract needed info into df \n",
    "with open('arxiv.xml', 'r') as f:\n",
    "\tfile = f.read() \n",
    " \n",
    "soup = BeautifulSoup(file, 'lxml')\n",
    "entries = [entry for entry in soup.find_all('entry')]\n",
    "titles = [title.text.strip() for title in soup.find_all('title')]\n",
    "summaries = [summary.text.strip() for summary in soup.find_all('summary')]\n",
    "links = [link.get('href') for link in soup.find_all('link', href=True, type=\"text/html\")]\n",
    "all_authors = []\n",
    "for entry in entries:\n",
    "\tauthors = [author.text.strip() for author in entry.find_all('name')]\n",
    "\tall_authors.append(authors)\n",
    "\n",
    "arxiv_df = pd.DataFrame(columns=['Title', 'Summary', 'Link', 'Authors'])\n",
    "arxiv_df['Title'] = pd.Series(titles[1:])\n",
    "arxiv_df['Summary'] = pd.Series(summaries)\n",
    "arxiv_df['Link'] = pd.Series(links)\n",
    "arxiv_df['Authors'] = pd.Series(all_authors)\n",
    "arxiv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Search RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open url and return local xml file \n",
    "url = 'https://www.google.com/alerts/feeds/11458941541514212856/10321436774246146597'\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n",
    "file = urllib.request.urlretrieve(url, 'google_news.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse xml file and extract info into df \n",
    "\n",
    "with open('google_news.xml', 'r') as f:\n",
    "\tfile = f.read() \n",
    " \n",
    "soup = BeautifulSoup(file, 'lxml')\n",
    "date = [date for date in soup.find_all('published')]\n",
    "titles = [title.text.strip() for title in soup.find_all('title')]\n",
    "content = [summary.text.strip() for summary in soup.find_all('content')]\n",
    "links = [link.get('href') for link in soup.find_all('link', href=True)]\n",
    "\n",
    "google_df = pd.DataFrame(columns=['Date', 'Title', 'Content', 'Link'])\n",
    "google_df['Date'] = pd.Series(date).astype('object')\n",
    "google_df['Title'] = pd.Series(titles).astype('string')\n",
    "google_df['Content'] = pd.Series(content).astype('string')\n",
    "google_df['Link'] = pd.Series(links[1:]).astype('string')\n",
    "google_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
