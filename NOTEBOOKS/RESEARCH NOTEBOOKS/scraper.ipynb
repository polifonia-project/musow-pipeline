{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "- This notebook includes tests for scraping different sources including github readme files and generic URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and path\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape github read me files \n",
    "def scrape_github(link_list):\n",
    "    github_pd = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if soup and soup.find('head') and soup.find('body') is not None:\n",
    "            title = ' '.join([h1.text for h1 in soup.find(id='readme').find_all('h1')])\n",
    "            text = ' '.join([p.text for p in soup.find(id='readme').find_all('p')]) \n",
    "            new_row = {'Title': title.str.lower().str.strip(), 'Description': text.str.lower().str.strip(), 'URL': URL.strip()}\n",
    "            github_pd = github_pd.append(new_row, ignore_index=True)\n",
    "    return github_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URLs for title and text \n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        try:\n",
    "            page = requests.get(URL)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            continue\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if soup and soup.find('head') and soup.find('body') is not None:\n",
    "            title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip()\n",
    "            text = ' '.join([p.text for p in soup.find('body').find_all('p')]).strip()\n",
    "            new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "            links = links.append(new_row, ignore_index=True)\n",
    "    return links"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
