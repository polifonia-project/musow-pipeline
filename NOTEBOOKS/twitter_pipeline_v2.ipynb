{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# musoW Twitter Pipeline\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "import pandas as pd\n",
    "#import custom functions\n",
    "from PYTHON_FILES.LogReg_Searches import LogRegSearches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions training set -> v2 = musow+mji descriptions vs summarized scrapes from twitter searches  \n",
    "archive_desc_training_v2 = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/archive_desc_training_v2.pkl')\n",
    "\n",
    "# twitter training set -> v1 = tweets from bigrams vs tweets for digital humanities and music company \n",
    "twitter_training_set_v1 = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/twitter_training_v1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training twitter and descriptions classifiers\n",
    "\n",
    "This is a ONE TIME operation. The models are pickled and loaded later to predict new results from LOGREG_RELEVANCE/MODELS folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time training on twitter\n",
    "twitter_training_model = LogRegSearches.train(twitter_training_set_v1, 'tweet', 'Target', 10, 'precision', 1000, 'twitter_pipeline', path)\n",
    "\n",
    "# one time training on resources\n",
    "resource_training_model = LogRegSearches.train(archive_desc_training_v2, 'Description', 'Target', 10, 'precision', 1000, 'resources_pipeline',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Twitter\n",
    "\n",
    "Calls Twitter API with a list of keywords and return results as raw csv and clean pickle in TWITTER_SEARCHES/RAW_SEARCHES folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load token\n",
    "token = 'AAAAAAAAAAAAAAAAAAAAAJgsNAEAAAAAQcsgbUnOJJmqmU483%2F8x6n9V1i8%3Df0qaEo9cV1sWP4eyNQ6E9s8BiRjvFTSN9mSqithe8uIXSNP68x'\n",
    "#a selection of keywords based on MJI and musoW datasets\n",
    "\n",
    "#Choose keywords  \n",
    "keywords = ['music archive', 'music collection']\n",
    "\n",
    "#search timeframe (if using custom search)\n",
    "start = ['2022-05-01T00:00:00.000Z', '2022-05-02T00:00:00.000Z', '2022-05-03T00:00:00.000Z', '2022-05-04T00:00:00.000Z', '2022-05-05T00:00:00.000Z', '2022-05-06T00:00:00.000Z', '2022-05-07T00:00:00.000Z']\n",
    "end = ['2022-05-01T23:59:59.000Z', '2022-05-02T23:59:59.000Z', '2022-05-03T23:59:59.000Z', '2022-05-04T23:59:59.000Z', '2022-05-05T23:59:59.000Z', '2022-05-06T23:59:59.000Z', '2022-05-07T23:59:59.000Z']\n",
    "\n",
    "#choose search option \n",
    "## search last week\n",
    "tweets = LogRegSearches.search_twitter_weekly(token, keywords, 50, 50)\n",
    "## search custom timeframe\n",
    "#LogRegSearches.search_twitter_custom(token, better_keywords, start, end, 500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all search results into a single dataframe \n",
    "tweets_to_classify = LogRegSearches.tweets_to_classify(path+'TWITTER_SEARCHES/RAW_SEARCHES/', f'{tweets[0][-16:]}.pkl')\n",
    "tweets_to_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run classification and get links from results\n",
    "predicted_tweets, twitter_link_list = LogRegSearches.predict_twitter(path, 'twitter_pipeline', tweets_to_classify, 'tweet', 1)\n",
    "predicted_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URL list and return a DF for resource classification\n",
    "scraped_links = LogRegSearches.scrape_links(twitter_link_list, predicted_tweets, f'{tweets[0][-16:]}_scrapes')\n",
    "scraped_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify web resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_resources = LogRegSearches.predict_resource(path, 'resources_pipeline', scraped_links, 'Description', 1, f'{tweets[0][-16:]}')\n",
    "predicted_resources"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
